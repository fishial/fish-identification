{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7089364f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Functions loaded ✓\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from IPython.display import display\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from openpyxl.styles import Font, PatternFill, Alignment, Border, Side\n",
        "from openpyxl.utils import get_column_letter\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "#  I/O\n",
        "# ---------------------------------------------------------------------------\n",
        "def load_results(path: Path) -> list[dict]:\n",
        "    with path.open() as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "#  Per-entry helpers\n",
        "# ---------------------------------------------------------------------------\n",
        "def check_top_k(entry: dict, k: int) -> bool:\n",
        "    gt = entry.get(\"gt_label\")\n",
        "    names = entry.get(\"names\") or []\n",
        "    accs = entry.get(\"accuracy\") or []\n",
        "    if not gt or not names:\n",
        "        return False\n",
        "    top_k = [names[i] for i in np.argsort(accs)[::-1][:k]]\n",
        "    return gt in top_k\n",
        "\n",
        "\n",
        "def top_n_names(entry: dict, n: int = 5) -> str:\n",
        "    names = entry.get(\"names\") or []\n",
        "    accs = entry.get(\"accuracy\") or []\n",
        "    if not names:\n",
        "        return \"\"\n",
        "    return \", \".join(names[i] for i in np.argsort(accs)[::-1][:n])\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "#  Normalise raw JSON → flat DataFrame (one row per annotation)\n",
        "# ---------------------------------------------------------------------------\n",
        "def normalize_results(entries: list[dict], tag: str) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for e in entries:\n",
        "        names = e.get(\"names\") or []\n",
        "        accs = e.get(\"accuracy\") or []\n",
        "        best = int(np.argmax(accs)) if accs else None\n",
        "        rows.append({\n",
        "            \"image_id\":       e.get(\"image_id\"),\n",
        "            \"ann_id\":         e.get(\"ann_id\"),\n",
        "            \"drawn_fish_id\":  e.get(\"drawn_fish_id\"),\n",
        "            \"gt_label\":       e.get(\"gt_label\"),\n",
        "            f\"{tag}_pred\":    names[best] if best is not None else None,\n",
        "            f\"{tag}_score\":   accs[best]  if best is not None else 0,\n",
        "            f\"{tag}_top5\":    top_n_names(e, 5),\n",
        "            \"top1_hit\": check_top_k(e, 1),\n",
        "            \"top3_hit\": check_top_k(e, 3),\n",
        "            \"top5_hit\": check_top_k(e, 5),\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "#  Per-class metrics\n",
        "# ---------------------------------------------------------------------------\n",
        "def per_class_metrics(df: pd.DataFrame, classes: list[str]) -> pd.DataFrame:\n",
        "    valid = df[df[\"gt_label\"].isin(classes)]\n",
        "    if valid.empty:\n",
        "        return pd.DataFrame(\n",
        "            0.0, index=classes,\n",
        "            columns=[\"support\", \"t1_abs\", \"t1_rec\", \"t3_abs\", \"t3_rec\", \"t5_abs\", \"t5_rec\"],\n",
        "        )\n",
        "    stats = valid.groupby(\"gt_label\").agg(\n",
        "        support=(\"gt_label\", \"count\"),\n",
        "        t1_abs=(\"top1_hit\", \"sum\"),\n",
        "        t3_abs=(\"top3_hit\", \"sum\"),\n",
        "        t5_abs=(\"top5_hit\", \"sum\"),\n",
        "    )\n",
        "    for k in (1, 3, 5):\n",
        "        stats[f\"t{k}_rec\"] = (stats[f\"t{k}_abs\"] / stats[\"support\"]).fillna(0)\n",
        "    return stats.reindex(classes).fillna(0)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "#  Build comparison table (per-class + weighted avg + macro avg)\n",
        "# ---------------------------------------------------------------------------\n",
        "def compare_models(df_old, df_new, classes):\n",
        "    m_old = per_class_metrics(df_old, classes)\n",
        "    m_new = per_class_metrics(df_new, classes)\n",
        "\n",
        "    c = pd.DataFrame(index=classes)\n",
        "    c[\"Samples\"] = m_new[\"support\"].astype(int)\n",
        "\n",
        "    for k in (1, 3, 5):\n",
        "        c[f\"v93 Top-{k} Hits\"]   = m_old[f\"t{k}_abs\"].astype(int)\n",
        "        c[f\"v10 Top-{k} Hits\"]   = m_new[f\"t{k}_abs\"].astype(int)\n",
        "        c[f\"v93 Top-{k} Recall\"] = m_old[f\"t{k}_rec\"]\n",
        "        c[f\"v10 Top-{k} Recall\"] = m_new[f\"t{k}_rec\"]\n",
        "        c[f\"Δ Top-{k} Recall\"]   = m_new[f\"t{k}_rec\"] - m_old[f\"t{k}_rec\"]\n",
        "\n",
        "    n = c[\"Samples\"].sum()\n",
        "\n",
        "    weighted = pd.Series(name=\"WEIGHTED AVG (Overall Accuracy)\", dtype=float)\n",
        "    weighted[\"Samples\"] = n\n",
        "    for k in (1, 3, 5):\n",
        "        weighted[f\"v93 Top-{k} Hits\"]   = c[f\"v93 Top-{k} Hits\"].sum()\n",
        "        weighted[f\"v10 Top-{k} Hits\"]   = c[f\"v10 Top-{k} Hits\"].sum()\n",
        "        weighted[f\"v93 Top-{k} Recall\"] = c[f\"v93 Top-{k} Hits\"].sum() / n if n else 0\n",
        "        weighted[f\"v10 Top-{k} Recall\"] = c[f\"v10 Top-{k} Hits\"].sum() / n if n else 0\n",
        "        weighted[f\"Δ Top-{k} Recall\"]   = weighted[f\"v10 Top-{k} Recall\"] - weighted[f\"v93 Top-{k} Recall\"]\n",
        "\n",
        "    macro = pd.Series(name=\"MACRO AVG (Mean Per-Class Recall)\", dtype=float)\n",
        "    macro[\"Samples\"] = n\n",
        "    for k in (1, 3, 5):\n",
        "        macro[f\"v93 Top-{k} Hits\"]   = c[f\"v93 Top-{k} Hits\"].sum()\n",
        "        macro[f\"v10 Top-{k} Hits\"]   = c[f\"v10 Top-{k} Hits\"].sum()\n",
        "        macro[f\"v93 Top-{k} Recall\"] = c[f\"v93 Top-{k} Recall\"].mean()\n",
        "        macro[f\"v10 Top-{k} Recall\"] = c[f\"v10 Top-{k} Recall\"].mean()\n",
        "        macro[f\"Δ Top-{k} Recall\"]   = macro[f\"v10 Top-{k} Recall\"] - macro[f\"v93 Top-{k} Recall\"]\n",
        "\n",
        "    body = c.sort_values(\"Δ Top-1 Recall\", ascending=False)\n",
        "    return pd.concat([body, pd.DataFrame([weighted, macro])])\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "#  Console summary\n",
        "# ---------------------------------------------------------------------------\n",
        "def print_summary(df_old, df_new, classes, title=\"\"):\n",
        "    m_old = per_class_metrics(df_old, classes)\n",
        "    m_new = per_class_metrics(df_new, classes)\n",
        "    n_cls = len(classes)\n",
        "    n_sam = int(m_new[\"support\"].sum())\n",
        "\n",
        "    w = 70\n",
        "    print(f\"\\n{'=' * w}\")\n",
        "    print(f\"  {title}  |  Classes: {n_cls}  |  Samples: {n_sam:,}\")\n",
        "    print(f\"{'=' * w}\")\n",
        "    print(f\"  {'Metric':<30} {'v93':>10} {'v10':>10} {'Δ':>10}\")\n",
        "    print(f\"  {'-' * 62}\")\n",
        "\n",
        "    for label, getter in [\n",
        "        (\"Weighted (micro)\", lambda m, k: m[f\"t{k}_abs\"].sum() / n_sam if n_sam else 0),\n",
        "        (\"Macro (per-class)\", lambda m, k: m[f\"t{k}_rec\"].mean()),\n",
        "    ]:\n",
        "        for k in (1, 3, 5):\n",
        "            v_old = getter(m_old, k)\n",
        "            v_new = getter(m_new, k)\n",
        "            d = v_new - v_old\n",
        "            sign = \"+\" if d >= 0 else \"\"\n",
        "            print(f\"  Top-{k} {label:<24} {v_old:>9.2%} {v_new:>9.2%} {sign}{d:>8.2%}\")\n",
        "        print()\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "#  Top-N errors per class for a given model\n",
        "# ---------------------------------------------------------------------------\n",
        "def top_errors_per_class(df: pd.DataFrame, pred_col: str, top5_col: str,\n",
        "                         n: int = 5) -> pd.DataFrame:\n",
        "    wrong = df[df[\"gt_label\"] != df[pred_col]].copy()\n",
        "\n",
        "    rows = []\n",
        "    for cls in sorted(df[\"gt_label\"].dropna().unique()):\n",
        "        cls_df = df[df[\"gt_label\"] == cls]\n",
        "        cls_total = len(cls_df)\n",
        "        cls_wrong = wrong[wrong[\"gt_label\"] == cls]\n",
        "        n_errors = len(cls_wrong)\n",
        "        error_rate = n_errors / cls_total if cls_total else 0\n",
        "\n",
        "        row = {\n",
        "            \"Class\": cls,\n",
        "            \"Total Samples\": cls_total,\n",
        "            \"Errors (Top-1)\": n_errors,\n",
        "            \"Error Rate\": error_rate,\n",
        "        }\n",
        "\n",
        "        if n_errors > 0:\n",
        "            top_wrong = cls_wrong[pred_col].value_counts().head(n)\n",
        "            for i, (pred, count) in enumerate(top_wrong.items(), 1):\n",
        "                row[f\"#{i} Wrong Pred\"] = pred\n",
        "                row[f\"#{i} Count\"] = count\n",
        "                row[f\"#{i} % of Errors\"] = count / n_errors\n",
        "        rows.append(row)\n",
        "\n",
        "    result = pd.DataFrame(rows)\n",
        "    result.sort_values(\"Errors (Top-1)\", ascending=False, inplace=True)\n",
        "    result.reset_index(drop=True, inplace=True)\n",
        "    return result\n",
        "\n",
        "\n",
        "print(\"Functions loaded ✓\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8a660562",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded:  v93 = 168,612 rows,  v10 = 168,612 rows\n",
            "Predicted classes — v93: 639,  v10: 775,  common: 638\n",
            "\n",
            "======================================================================\n",
            "  COMMON CLASSES (intersection)  |  Classes: 638  |  Samples: 149,304\n",
            "======================================================================\n",
            "  Metric                                v93        v10          Δ\n",
            "  --------------------------------------------------------------\n",
            "  Top-1 Weighted (micro)            96.16%    95.90%   -0.26%\n",
            "  Top-3 Weighted (micro)            98.04%    98.69% +   0.64%\n",
            "  Top-5 Weighted (micro)            98.19%    98.97% +   0.78%\n",
            "\n",
            "  Top-1 Macro (per-class)           95.26%    96.69% +   1.44%\n",
            "  Top-3 Macro (per-class)           97.53%    98.96% +   1.43%\n",
            "  Top-5 Macro (per-class)           97.75%    99.18% +   1.43%\n",
            "\n",
            "\n",
            "======================================================================\n",
            "  v93 LABEL SET  |  Classes: 639  |  Samples: 149,304\n",
            "======================================================================\n",
            "  Metric                                v93        v10          Δ\n",
            "  --------------------------------------------------------------\n",
            "  Top-1 Weighted (micro)            96.16%    95.90%   -0.26%\n",
            "  Top-3 Weighted (micro)            98.04%    98.69% +   0.64%\n",
            "  Top-5 Weighted (micro)            98.19%    98.97% +   0.78%\n",
            "\n",
            "  Top-1 Macro (per-class)           95.11%    96.54% +   1.44%\n",
            "  Top-3 Macro (per-class)           97.38%    98.80% +   1.42%\n",
            "  Top-5 Macro (per-class)           97.60%    99.03% +   1.43%\n",
            "\n",
            "\n",
            "======================================================================\n",
            "  v10 LABEL SET  |  Classes: 775  |  Samples: 156,145\n",
            "======================================================================\n",
            "  Metric                                v93        v10          Δ\n",
            "  --------------------------------------------------------------\n",
            "  Top-1 Weighted (micro)            91.95%    95.84% +   3.89%\n",
            "  Top-3 Weighted (micro)            93.75%    98.71% +   4.96%\n",
            "  Top-5 Weighted (micro)            93.89%    99.00% +   5.11%\n",
            "\n",
            "  Top-1 Macro (per-class)           78.42%    96.26% +  17.84%\n",
            "  Top-3 Macro (per-class)           80.29%    99.00% +  18.71%\n",
            "  Top-5 Macro (per-class)           80.47%    99.29% +  18.81%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "#  Load data & build normalised DataFrames\n",
        "# ---------------------------------------------------------------------------\n",
        "v93_path = Path(\"inference_results_v93.json\")\n",
        "v10_path = Path(\"inference_results_v10.json\")\n",
        "output_path = Path(\"validation_comparison_v93_vs_v10.xlsx\")\n",
        "\n",
        "df_v93 = normalize_results(load_results(v93_path), \"v93\")\n",
        "df_v10 = normalize_results(load_results(v10_path), \"v10\")\n",
        "\n",
        "cls_v93 = sorted(df_v93[\"v93_pred\"].dropna().unique())\n",
        "cls_v10 = sorted(df_v10[\"v10_pred\"].dropna().unique())\n",
        "common  = sorted(set(cls_v93) & set(cls_v10))\n",
        "\n",
        "print(f\"Loaded:  v93 = {len(df_v93):,} rows,  v10 = {len(df_v10):,} rows\")\n",
        "print(f\"Predicted classes — v93: {len(cls_v93)},  v10: {len(cls_v10)},  common: {len(common)}\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "#  Console summaries for each class set\n",
        "# ---------------------------------------------------------------------------\n",
        "print_summary(df_v93, df_v10, common,  \"COMMON CLASSES (intersection)\")\n",
        "print_summary(df_v93, df_v10, cls_v93, \"v93 LABEL SET\")\n",
        "print_summary(df_v93, df_v10, cls_v10, \"v10 LABEL SET\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "00c703db",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Excel saved → validation_comparison_v93_vs_v10.xlsx\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "#  Excel export with formatting\n",
        "# ---------------------------------------------------------------------------\n",
        "SETS = {\n",
        "    \"Common_Classes\": common,\n",
        "    \"v93_Set\":        cls_v93,\n",
        "    \"v10_Set\":        cls_v10,\n",
        "}\n",
        "\n",
        "# ---- styles ----\n",
        "HEADER_FILL = PatternFill(\"solid\", fgColor=\"4472C4\")\n",
        "HEADER_FONT = Font(bold=True, size=11, color=\"FFFFFF\")\n",
        "SUMMARY_FILL = PatternFill(\"solid\", fgColor=\"FFF2CC\")\n",
        "SUMMARY_FONT = Font(bold=True, size=11)\n",
        "GREEN_FILL = PatternFill(\"solid\", fgColor=\"C6EFCE\")\n",
        "GREEN_FONT = Font(color=\"006100\")\n",
        "RED_FILL   = PatternFill(\"solid\", fgColor=\"FFC7CE\")\n",
        "RED_FONT   = Font(color=\"9C0006\")\n",
        "_s = Side(style=\"thin\")\n",
        "THIN = Border(left=_s, right=_s, top=_s, bottom=_s)\n",
        "\n",
        "with pd.ExcelWriter(output_path, engine=\"openpyxl\") as writer:\n",
        "\n",
        "    # ── 1. Summary sheet ─────────────────────────────────────────────────\n",
        "    summary_rows = []\n",
        "    for sname, classes in SETS.items():\n",
        "        mo = per_class_metrics(df_v93, classes)\n",
        "        mn = per_class_metrics(df_v10, classes)\n",
        "        n = int(mn[\"support\"].sum())\n",
        "        row = {\"Set\": sname, \"Classes\": len(classes), \"Samples\": n}\n",
        "        for k in (1, 3, 5):\n",
        "            oh, nh = int(mo[f\"t{k}_abs\"].sum()), int(mn[f\"t{k}_abs\"].sum())\n",
        "            row[f\"v93 Top-{k} Acc\"]      = oh / n if n else 0\n",
        "            row[f\"v10 Top-{k} Acc\"]      = nh / n if n else 0\n",
        "            row[f\"Δ Top-{k} Acc\"]        = row[f\"v10 Top-{k} Acc\"] - row[f\"v93 Top-{k} Acc\"]\n",
        "            row[f\"v93 Top-{k} Macro\"]    = mo[f\"t{k}_rec\"].mean()\n",
        "            row[f\"v10 Top-{k} Macro\"]    = mn[f\"t{k}_rec\"].mean()\n",
        "            row[f\"Δ Top-{k} Macro\"]      = row[f\"v10 Top-{k} Macro\"] - row[f\"v93 Top-{k} Macro\"]\n",
        "        summary_rows.append(row)\n",
        "    pd.DataFrame(summary_rows).to_excel(writer, sheet_name=\"Summary\", index=False)\n",
        "\n",
        "    # ── 2. Per-class comparison sheets ────────────────────────────────────\n",
        "    for sname, classes in SETS.items():\n",
        "        compare_models(df_v93, df_v10, classes).to_excel(writer, sheet_name=sname)\n",
        "\n",
        "    # ── 3. Raw data (merged) ─────────────────────────────────────────────\n",
        "    raw = df_v93.merge(\n",
        "        df_v10,\n",
        "        on=[\"image_id\", \"ann_id\", \"drawn_fish_id\", \"gt_label\"],\n",
        "        suffixes=(\"_v93\", \"_v10\"),\n",
        "        how=\"outer\",\n",
        "    )\n",
        "    raw.to_excel(writer, sheet_name=\"Raw_Data\", index=False)\n",
        "\n",
        "    # ── 4. Top-5 errors per class for each model ────────────────────────\n",
        "    err_v93 = top_errors_per_class(df_v93, \"v93_pred\", \"v93_top5\")\n",
        "    err_v10 = top_errors_per_class(df_v10, \"v10_pred\", \"v10_top5\")\n",
        "    err_v93.to_excel(writer, sheet_name=\"v93_Top_Errors\", index=False)\n",
        "    err_v10.to_excel(writer, sheet_name=\"v10_Top_Errors\", index=False)\n",
        "\n",
        "    # ── 5. Class dictionaries ─────────────────────────────────────────────\n",
        "    pd.DataFrame({\n",
        "        \"v93 classes\":    pd.Series(cls_v93),\n",
        "        \"v10 classes\":    pd.Series(cls_v10),\n",
        "        \"common classes\": pd.Series(common),\n",
        "    }).to_excel(writer, sheet_name=\"Class_Lists\", index=False)\n",
        "\n",
        "    # ==================================================================\n",
        "    #  Formatting pass\n",
        "    # ==================================================================\n",
        "    wb = writer.book\n",
        "    for sname in wb.sheetnames:\n",
        "        ws = wb[sname]\n",
        "\n",
        "        # --- header row ---\n",
        "        for cell in ws[1]:\n",
        "            cell.font = HEADER_FONT\n",
        "            cell.fill = HEADER_FILL\n",
        "            cell.alignment = Alignment(horizontal=\"center\", wrap_text=True)\n",
        "            cell.border = THIN\n",
        "\n",
        "        # --- freeze ---\n",
        "        ws.freeze_panes = \"B2\" if sname in SETS else \"A2\"\n",
        "\n",
        "        # --- auto-width ---\n",
        "        for ci, col in enumerate(ws.columns, 1):\n",
        "            width = max((len(str(c.value or \"\")) for c in col), default=8)\n",
        "            ws.column_dimensions[get_column_letter(ci)].width = min(width + 3, 42)\n",
        "\n",
        "        # --- comparison sheets: recall %, delta coloring, summary rows ---\n",
        "        if sname in SETS:\n",
        "            hdrs = [c.value for c in ws[1]]\n",
        "            delta_ci = {i + 1 for i, h in enumerate(hdrs) if h and \"Δ\" in str(h)}\n",
        "            recall_ci = {i + 1 for i, h in enumerate(hdrs) if h and \"Recall\" in str(h)}\n",
        "            pct_ci = delta_ci | recall_ci\n",
        "\n",
        "            for row in ws.iter_rows(min_row=2, max_row=ws.max_row):\n",
        "                is_summary = row[0].value and any(\n",
        "                    tag in str(row[0].value) for tag in (\"AVG\", \"WEIGHTED\", \"MACRO\")\n",
        "                )\n",
        "                for cell in row:\n",
        "                    cell.border = THIN\n",
        "                    if cell.column in pct_ci and isinstance(cell.value, (int, float)):\n",
        "                        cell.number_format = \"0.00%\"\n",
        "                    if cell.column in delta_ci and isinstance(cell.value, (int, float)):\n",
        "                        if cell.value > 0.001:\n",
        "                            cell.fill, cell.font = GREEN_FILL, GREEN_FONT\n",
        "                        elif cell.value < -0.001:\n",
        "                            cell.fill, cell.font = RED_FILL, RED_FONT\n",
        "                    if is_summary and cell.column not in delta_ci:\n",
        "                        cell.fill = SUMMARY_FILL\n",
        "                        cell.font = SUMMARY_FONT\n",
        "\n",
        "        # --- Summary sheet: % format + delta coloring ---\n",
        "        if sname == \"Summary\":\n",
        "            hdrs = [c.value for c in ws[1]]\n",
        "            pct_ci = {i + 1 for i, h in enumerate(hdrs)\n",
        "                      if h and any(t in str(h) for t in (\"Acc\", \"Macro\", \"Δ\"))}\n",
        "            delta_ci = {i + 1 for i, h in enumerate(hdrs) if h and \"Δ\" in str(h)}\n",
        "\n",
        "            for row in ws.iter_rows(min_row=2, max_row=ws.max_row):\n",
        "                for cell in row:\n",
        "                    cell.border = THIN\n",
        "                    if cell.column in pct_ci and isinstance(cell.value, (int, float)):\n",
        "                        cell.number_format = \"0.00%\"\n",
        "                    if cell.column in delta_ci and isinstance(cell.value, (int, float)):\n",
        "                        if cell.value > 0.001:\n",
        "                            cell.fill, cell.font = GREEN_FILL, GREEN_FONT\n",
        "                        elif cell.value < -0.001:\n",
        "                            cell.fill, cell.font = RED_FILL, RED_FONT\n",
        "\n",
        "        # --- Top-Errors sheets: % format + error-rate coloring ---\n",
        "        if sname in (\"v93_Top_Errors\", \"v10_Top_Errors\"):\n",
        "            hdrs = [c.value for c in ws[1]]\n",
        "            pct_ci = {i + 1 for i, h in enumerate(hdrs)\n",
        "                      if h and (\"Error Rate\" in str(h) or \"% of Errors\" in str(h))}\n",
        "            err_rate_ci = {i + 1 for i, h in enumerate(hdrs)\n",
        "                          if h and \"Error Rate\" == str(h)}\n",
        "\n",
        "            for row in ws.iter_rows(min_row=2, max_row=ws.max_row):\n",
        "                for cell in row:\n",
        "                    cell.border = THIN\n",
        "                    if cell.column in pct_ci and isinstance(cell.value, (int, float)):\n",
        "                        cell.number_format = \"0.00%\"\n",
        "                    if cell.column in err_rate_ci and isinstance(cell.value, (int, float)):\n",
        "                        if cell.value > 0.3:\n",
        "                            cell.fill, cell.font = RED_FILL, RED_FONT\n",
        "                        elif cell.value < 0.05:\n",
        "                            cell.fill, cell.font = GREEN_FILL, GREEN_FONT\n",
        "\n",
        "print(f\"Excel saved → {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6758d2eb",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "339e58cf",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "284d3d6c",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
