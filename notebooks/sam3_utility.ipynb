{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7feef87",
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone as fo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dbad757",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = fo.load_dataset(\"segmentation_dataset_v0.10\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f35af0b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "for sample in dataset:\n",
        "    \n",
        "    gt_points = [poly['points'][0] for poly in sample['General body shape']['polylines']]\n",
        "\n",
        "    sam3_points = [poly['points'][0] for poly in sample['sam3_segmentation']['polylines']]\n",
        "    if len(gt_points) > 1:\n",
        "        print(len(gt_points))\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e221f1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from shapely.geometry import Polygon\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def calculate_iou(poly1_points, poly2_points):\n",
        "    \"\"\"Вычисляет IoU между двумя полигонами\"\"\"\n",
        "    try:\n",
        "        poly1 = Polygon(poly1_points)\n",
        "        poly2 = Polygon(poly2_points)\n",
        "        \n",
        "        if not poly1.is_valid or not poly2.is_valid:\n",
        "            return 0.0\n",
        "        \n",
        "        intersection = poly1.intersection(poly2).area\n",
        "        union = poly1.union(poly2).area\n",
        "        \n",
        "        if union == 0:\n",
        "            return 0.0\n",
        "        \n",
        "        return intersection / union\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def find_unmatched_detections(gt_polylines, sam3_polylines, iou_threshold=0.2):\n",
        "    \"\"\"\n",
        "    Находит непарные детекции (те, у которых нет хорошего матча)\n",
        "    Возвращает: (количество непарных GT, количество непарных SAM3, средний IoU)\n",
        "    \"\"\"\n",
        "    if not gt_polylines or not sam3_polylines:\n",
        "        # Если один из списков пустой, все детекции в другом списке непарные\n",
        "        return len(gt_polylines or []), len(sam3_polylines or []), 0.0\n",
        "    \n",
        "    gt_points_list = [poly['points'][0] for poly in gt_polylines]\n",
        "    sam3_points_list = [poly['points'][0] for poly in sam3_polylines]\n",
        "    \n",
        "    # Вычисляем IoU между всеми парами\n",
        "    iou_matrix = np.zeros((len(gt_points_list), len(sam3_points_list)))\n",
        "    for i, gt_points in enumerate(gt_points_list):\n",
        "        for j, sam3_points in enumerate(sam3_points_list):\n",
        "            iou_matrix[i, j] = calculate_iou(gt_points, sam3_points)\n",
        "    \n",
        "    # Находим лучший матч для каждого GT\n",
        "    unmatched_gt = 0\n",
        "    for i in range(len(gt_points_list)):\n",
        "        best_iou = np.max(iou_matrix[i, :])\n",
        "        if best_iou < iou_threshold:\n",
        "            unmatched_gt += 1\n",
        "    \n",
        "    # Finding the best match for every SAM3\n",
        "    unmatched_sam3 = 0\n",
        "    for j in range(len(sam3_points_list)):\n",
        "        best_iou = np.max(iou_matrix[:, j])\n",
        "        if best_iou < iou_threshold:\n",
        "            unmatched_sam3 += 1\n",
        "    \n",
        "    avg_iou = np.mean(iou_matrix) if iou_matrix.size > 0 else 0.0\n",
        "    \n",
        "    return unmatched_gt, unmatched_sam3, avg_iou\n",
        "\n",
        "for sample in tqdm(dataset, desc=\"Calculating IoU and finding unmatched detections\"):\n",
        "    gt_polylines = sample['General body shape']['polylines'] if sample['General body shape'] else []\n",
        "    sam3_polylines = sample['sam3_segmentation']['polylines'] if sample['sam3_segmentation'] else []\n",
        "    \n",
        "    unmatched_gt, unmatched_sam3, avg_iou = find_unmatched_detections(\n",
        "        gt_polylines, sam3_polylines, iou_threshold=0.2\n",
        "    )\n",
        "    \n",
        "    sample['unmatched_gt_count'] = unmatched_gt\n",
        "    sample['unmatched_sam3_count'] = unmatched_sam3\n",
        "    sample['avg_iou'] = avg_iou\n",
        "    sample['has_unmatched'] = (unmatched_gt > 0) or (unmatched_sam3 > 0)\n",
        "    sample.save()\n",
        "\n",
        "print(\"Analysis complete. Fields added:\")\n",
        "print(\"- unmatched_gt_count: number of GT polygons without a good match\")\n",
        "print(\"- unmatched_sam3_count: number of SAM3 polygons without a good match\")\n",
        "print(\"- avg_iou: average IoU between all pairs\")\n",
        "print(\"- has_unmatched: whether there are unmatched detections\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ea22a03",
      "metadata": {},
      "outputs": [],
      "source": [
        "# View 1: SAM3 found detections that did not find a match in GT (IoU < 0.2)\n",
        "from fiftyone import ViewField as F\n",
        "\n",
        "view_sam3_unmatched = dataset.match(F(\"unmatched_sam3_count\") > 0)\n",
        "\n",
        "print(f\"Samples where SAM3 found detections without a good match in GT: {len(view_sam3_unmatched)}\")\n",
        "print(f\"Total unmatched SAM3 detections: {sum([s['unmatched_sam3_count'] for s in view_sam3_unmatched])}\")\n",
        "session1 = fo.launch_app(view_sam3_unmatched)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03551a3f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# View 2: GT detections that did not find a match in SAM3 (IoU < 0.2)\n",
        "view_gt_unmatched = dataset.match(F(\"unmatched_gt_count\") > 0)\n",
        "\n",
        "print(f\"Samples where GT detections did not find a good match in SAM3: {len(view_gt_unmatched)}\")\n",
        "print(f\"Total unmatched GT detections: {sum([s['unmatched_gt_count'] for s in view_gt_unmatched])}\")\n",
        "session2 = fo.launch_app(view_gt_unmatched)\n",
        "\n",
        "# View 3: All samples with any mismatches (GT or SAM3)\n",
        "view_all_unmatched = dataset.match(F(\"has_unmatched\") == True)\n",
        "\n",
        "print(f\"Total samples with mismatches: {len(view_all_unmatched)}\")\n",
        "print(f\"Total unmatched GT: {sum([s['unmatched_gt_count'] for s in view_all_unmatched])}\")\n",
        "print(f\"Total unmatched SAM3: {sum([s['unmatched_sam3_count'] for s in view_all_unmatched])}\")\n",
        "session3 = fo.launch_app(view_all_unmatched)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cebf5551",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2267535",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54b0fddb",
      "metadata": {},
      "outputs": [],
      "source": [
        "from shapely.geometry import Polygon\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def get_polygon_area(points):\n",
        "    \"\"\"Calculates the area of a polygon (normalized)\"\"\"\n",
        "    try:\n",
        "        poly = Polygon(points)\n",
        "        return poly.area if poly.is_valid else 0.0\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def iou(poly1_points, poly2_points):\n",
        "    \"\"\"Calculates Intersection over Union of two polygons\"\"\"\n",
        "    poly1 = Polygon(poly1_points)\n",
        "    poly2 = Polygon(poly2_points)\n",
        "    if not poly1.is_valid or not poly2.is_valid:\n",
        "        return 0.0\n",
        "    inter = poly1.intersection(poly2).area\n",
        "    union = poly1.union(poly2).area\n",
        "    if union == 0:\n",
        "        return 0.0\n",
        "    return inter / union\n",
        "\n",
        "sam3_areas = []\n",
        "gt_areas = []\n",
        "unique_sam3_areas = []\n",
        "\n",
        "iou_threshold = 0.5\n",
        "\n",
        "for sample in tqdm(dataset, desc=\"Analyzing SAM3 and GT polygons\"):\n",
        "    sam3_polys = []\n",
        "    gt_polys = []\n",
        "\n",
        "    # SAM3 polygons\n",
        "    if sample['sam3_segmentation'] and sample['sam3_segmentation']['polylines']:\n",
        "        for poly in sample['sam3_segmentation']['polylines']:\n",
        "            points = poly['points'][0]\n",
        "            area = get_polygon_area(points)\n",
        "            if area > 0:\n",
        "                sam3_areas.append(area)\n",
        "                sam3_polys.append(points)\n",
        "    \n",
        "    # GT polygons\n",
        "    if sample['General body shape'] and sample['General body shape']['polylines']:\n",
        "        for poly in sample['General body shape']['polylines']:\n",
        "            points = poly['points'][0]\n",
        "            area = get_polygon_area(points)\n",
        "            if area > 0:\n",
        "                gt_areas.append(area)\n",
        "                gt_polys.append(points)\n",
        "    \n",
        "    # Unique SAM3 polygons (no matching GT with IOU >= 0.5)\n",
        "    for s_poly in sam3_polys:\n",
        "        max_iou = 0.0\n",
        "        for g_poly in gt_polys:\n",
        "            max_iou = max(max_iou, iou(s_poly, g_poly))\n",
        "        if max_iou < iou_threshold:\n",
        "            unique_sam3_areas.append(get_polygon_area(s_poly))\n",
        "\n",
        "# Convert to numpy arrays\n",
        "sam3_areas = np.array(sam3_areas)\n",
        "gt_areas = np.array(gt_areas)\n",
        "unique_sam3_areas = np.array(unique_sam3_areas)\n",
        "\n",
        "def print_detailed_stats(name, areas):\n",
        "    print(f\"=== {name} ===\")\n",
        "    print(f\"Total polygons: {len(areas)}\")\n",
        "    if len(areas) == 0:\n",
        "        return\n",
        "    print(f\"Minimum: {areas.min():.6f}\")\n",
        "    print(f\"Maximum: {areas.max():.6f}\")\n",
        "    print(f\"Mean: {areas.mean():.6f}\")\n",
        "    print(f\"Median: {np.median(areas):.6f}\")\n",
        "    \n",
        "    # Additional quantile points\n",
        "    quantiles = [0.1, 0.25, 0.5, 1, 5, 10, 25, 50, 75, 90, 95, 99]\n",
        "    for q in quantiles:\n",
        "        print(f\"{q}% quantile: {np.percentile(areas, q):.6f}\")\n",
        "    \n",
        "    # Small intervals 0-0.1\n",
        "    intervals = np.arange(0, 0.11, 0.01)\n",
        "    counts, _ = np.histogram(areas, bins=intervals)\n",
        "    print(\"\\nDistribution in range 0-0.1:\")\n",
        "    for i in range(len(counts)):\n",
        "        print(f\"  {intervals[i]:.2f}-{intervals[i+1]:.2f}: {counts[i]} ({counts[i]/len(areas)*100:.2f}%)\")\n",
        "\n",
        "# Output statistics\n",
        "print_detailed_stats(\"SAM3\", sam3_areas)\n",
        "print_detailed_stats(\"GT\", gt_areas)\n",
        "print_detailed_stats(\"Unique SAM3 (IOU < 0.5)\", unique_sam3_areas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2239f79",
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_detailed_stats(name, areas):\n",
        "    print(f\"=== {name} ===\")\n",
        "    print(f\"Total polygons: {len(areas)}\")\n",
        "    if len(areas) == 0:\n",
        "        return\n",
        "    print(f\"Minimum: {areas.min():.6f}\")\n",
        "    print(f\"Maximum: {areas.max():.6f}\")\n",
        "    print(f\"Mean: {areas.mean():.6f}\")\n",
        "    print(f\"Median: {np.median(areas):.6f}\")\n",
        "    \n",
        "    # Additional quantile points\n",
        "    quantiles = [0.1, 0.25, 0.5, 1, 5, 10, 25, 50, 75, 90, 95, 99]\n",
        "    for q in quantiles:\n",
        "        print(f\"{q}% quantile: {np.percentile(areas, q):.6f}\")\n",
        "    \n",
        "    # Small intervals 0-0.7\n",
        "    intervals = np.arange(0, 0.71, 0.01)\n",
        "    counts, _ = np.histogram(areas, bins=intervals)\n",
        "    print(\"\\nDistribution in range 0-0.7:\")\n",
        "    for i in range(len(counts)):\n",
        "        # Calculate percentage for each bin\n",
        "        percentage = counts[i] / len(areas) * 100\n",
        "        print(f\"  {intervals[i]:.2f}-{intervals[i+1]:.2f}: {counts[i]} ({percentage:.2f}%)\")\n",
        "\n",
        "# Output statistics\n",
        "print_detailed_stats(\"SAM3\", sam3_areas)\n",
        "print_detailed_stats(\"GT\", gt_areas)\n",
        "print_detailed_stats(\"Unique SAM3 (IOU < 0.5)\", unique_sam3_areas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d1fe5b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Area distribution visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# 1. SAM3 Histogram (all data)\n",
        "axes[0, 0].hist(sam3_areas, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
        "axes[0, 0].set_xlabel('Area (normalized)')\n",
        "axes[0, 0].set_ylabel('Count')\n",
        "axes[0, 0].set_title('SAM3 Polygon Area Distribution')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. SAM3 Histogram (small polygons only, up to 1% quantile)\n",
        "threshold_1pct = np.percentile(sam3_areas, 1)\n",
        "small_sam3 = sam3_areas[sam3_areas <= threshold_1pct * 10]  # up to 10x the 1% quantile\n",
        "axes[0, 1].hist(small_sam3, bins=50, alpha=0.7, color='red', edgecolor='black')\n",
        "axes[0, 1].axvline(threshold_1pct, color='green', linestyle='--', linewidth=2, label=f'1% Quantile: {threshold_1pct:.6f}')\n",
        "axes[0, 1].set_xlabel('Area (normalized)')\n",
        "axes[0, 1].set_ylabel('Count')\n",
        "axes[0, 1].set_title('Distribution of Small SAM3 Polygons')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Comparison: SAM3 vs GT\n",
        "axes[1, 0].hist([sam3_areas, gt_areas], bins=50, alpha=0.6, \n",
        "                label=['SAM3', 'GT'], color=['blue', 'green'], edgecolor='black')\n",
        "axes[1, 0].set_xlabel('Area (normalized)')\n",
        "axes[1, 0].set_ylabel('Count')\n",
        "axes[1, 0].set_title('Comparison: SAM3 vs GT')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Box plot for comparison\n",
        "axes[1, 1].boxplot([sam3_areas, gt_areas], labels=['SAM3', 'GT'])\n",
        "axes[1, 1].set_ylabel('Area (normalized)')\n",
        "axes[1, 1].set_title('Box Plot: SAM3 vs GT')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n=== FILTERING THRESHOLD RECOMMENDATIONS ===\")\n",
        "print(f\"Smallest 1%: area < {np.percentile(sam3_areas, 1):.6f}\")\n",
        "print(f\"Smallest 5%: area < {np.percentile(sam3_areas, 5):.6f}\")\n",
        "print(f\"Smallest 10%: area < {np.percentile(sam3_areas, 10):.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "178ff9d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analysis: how many polygons will be filtered at different thresholds\n",
        "thresholds = [\n",
        "    np.percentile(sam3_areas, 1),\n",
        "    np.percentile(sam3_areas, 5),\n",
        "    np.percentile(sam3_areas, 10),\n",
        "    0.001,  # 0.1% of the image\n",
        "    0.005,  # 0.5% of the image\n",
        "    0.01,   # 1% of the image\n",
        "]\n",
        "\n",
        "print(\"=== NUMBER OF FILTERED POLYGONS AT VARIOUS THRESHOLDS ===\\n\")\n",
        "for threshold in thresholds:\n",
        "    count = np.sum(sam3_areas < threshold)\n",
        "    percent = (count / len(sam3_areas)) * 100\n",
        "    print(f\"Threshold {threshold:.6f}: {count:4d} polygons ({percent:5.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "408600d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating a new dataset with merged polygons\n",
        "from shapely.geometry import Polygon\n",
        "\n",
        "def get_polygon_area(points):\n",
        "    \"\"\"Calculates the area of a polygon (normalized)\"\"\"\n",
        "    try:\n",
        "        poly = Polygon(points)\n",
        "        return poly.area if poly.is_valid else 0.0\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def merge_detections(gt_polylines, sam3_polylines, iou_threshold=0.2, min_area=0.001):\n",
        "    \"\"\"\n",
        "    Merges GT and unique SAM3 polygons\n",
        "    - Keeps all GT polygons\n",
        "    - Adds SAM3 polygons that do not match GT (IoU < threshold)\n",
        "    - Filters out SAM3 polygons that are too small (area < min_area)\n",
        "    \"\"\"\n",
        "    merged_polylines = []\n",
        "    \n",
        "    # Add all GT polygons\n",
        "    if gt_polylines:\n",
        "        merged_polylines.extend(gt_polylines)\n",
        "    \n",
        "    # Add unique SAM3 polygons (not matching any GT)\n",
        "    if sam3_polylines:\n",
        "        gt_points_list = [poly['points'][0] for poly in gt_polylines] if gt_polylines else []\n",
        "        \n",
        "        for sam3_poly in sam3_polylines:\n",
        "            sam3_points = sam3_poly['points'][0]\n",
        "            \n",
        "            # Check the area\n",
        "            area = get_polygon_area(sam3_points)\n",
        "            if area < min_area:\n",
        "                continue  # Skip polygons that are too small\n",
        "            \n",
        "            # Check if there is a good match with GT\n",
        "            has_match = False\n",
        "            for gt_points in gt_points_list:\n",
        "                iou = calculate_iou(gt_points, sam3_points)\n",
        "                if iou >= iou_threshold:\n",
        "                    has_match = True\n",
        "                    break\n",
        "            \n",
        "            # If no good match with GT exists, add this SAM3 polygon\n",
        "            if not has_match:\n",
        "                merged_polylines.append(sam3_poly)\n",
        "    \n",
        "    return merged_polylines\n",
        "\n",
        "print(\"Polygon merging functions have been created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "316d70dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating a new dataset with merged polygons\n",
        "import fiftyone as fo\n",
        "\n",
        "# Dataset cloning configuration\n",
        "new_dataset_name = \"segmentation_merged_v0.1_full\"\n",
        "\n",
        "# Delete if it already exists\n",
        "if new_dataset_name in fo.list_datasets():\n",
        "    print(f\"Deleting dataset: {new_dataset_name}\")\n",
        "    fo.delete_dataset(new_dataset_name)\n",
        "\n",
        "# Create the new dataset\n",
        "new_dataset = dataset.clone(new_dataset_name)\n",
        "new_dataset.persistent = True\n",
        "print(f\"New dataset created: {new_dataset_name}\")\n",
        "print(f\"Number of samples: {len(new_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1442166c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Updating the \"General body shape\" field with merged polygons\n",
        "from tqdm import tqdm\n",
        "\n",
        "min_area_threshold = 0.005  # Minimum area for SAM3 polygons (0.1% of the image)\n",
        "\n",
        "stats = {\n",
        "    'total_samples': 0,\n",
        "    'gt_polygons': 0,\n",
        "    'sam3_added': 0,\n",
        "    'sam3_filtered_small': 0,\n",
        "    'sam3_filtered_matched': 0\n",
        "}\n",
        "\n",
        "for sample in tqdm(new_dataset, desc=\"Merging polygons\"):\n",
        "    gt_polylines = sample['General body shape']['polylines'] if sample['General body shape'] else []\n",
        "    sam3_polylines = sample['sam3_segmentation']['polylines'] if sample['sam3_segmentation'] else []\n",
        "    \n",
        "    original_gt_count = len(gt_polylines)\n",
        "    original_sam3_count = len(sam3_polylines)\n",
        "    \n",
        "    # Merge polygons\n",
        "    merged_polylines = merge_detections(\n",
        "        gt_polylines, \n",
        "        sam3_polylines, \n",
        "        iou_threshold=0.1,\n",
        "        min_area=min_area_threshold\n",
        "    )\n",
        "    \n",
        "    # Update \"General body shape\" field\n",
        "    if sample['General body shape']:\n",
        "        sample['General body shape']['polylines'] = merged_polylines\n",
        "    else:\n",
        "        # Create a new field if it didn't exist\n",
        "        sample['General body shape'] = fo.Polylines(polylines=merged_polylines)\n",
        "    \n",
        "    # Statistics\n",
        "    sam3_added = len(merged_polylines) - original_gt_count\n",
        "    stats['total_samples'] += 1\n",
        "    stats['gt_polygons'] += original_gt_count\n",
        "    stats['sam3_added'] += sam3_added\n",
        "    \n",
        "    sample.save()\n",
        "\n",
        "print(\"Processing complete!\")\n",
        "print(f\"Samples processed: {stats['total_samples']}\")\n",
        "print(f\"GT polygons: {stats['gt_polygons']}\")\n",
        "print(f\"SAM3 polygons added: {stats['sam3_added']}\")\n",
        "print(f\"\\nNew dataset: {new_dataset_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f421ca5e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization of the new dataset\n",
        "session_merged = fo.launch_app(new_dataset)\n",
        "\n",
        "print(f\"Opened new dataset: {new_dataset_name}\")\n",
        "print(f\"The 'General body shape' field now contains:\")\n",
        "print(f\"  - All original GT polygons\")\n",
        "print(f\"  - Unique SAM3 polygons (IoU < 0.2 with GT)\")\n",
        "print(f\"  - Filtered out small SAM3 polygons (area < {min_area_threshold})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8330e117",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "import argparse\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def read_json_file(file_path: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Read a JSON file and return its content as a dictionary.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the JSON file.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: The loaded JSON data.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "coco_path = \"/home/fishial/Fishial/dataset/EXPORT_V_0_9/Fishial_Export_Jan_08_2026_04_14_Production_AI_Gen_All_Verified.json\"\n",
        "images_data_folder = \"/home/fishial/Fishial/dataset/EXPORT_V_0_8/data/data\"\n",
        "\n",
        "# print(f\"Start reading COCO file: {coco_path}\")\n",
        "# data = read_json_file(coco_path)\n",
        "# print(\"Finished reading COCO file\")\n",
        "\n",
        "\n",
        "print(f\"images_data_folder: {images_data_folder}\")\n",
        "# Get list of already downloaded images\n",
        "downloaded_files = [\n",
        "    f for f in os.listdir(images_data_folder)\n",
        "    if os.path.isfile(os.path.join(images_data_folder, f))\n",
        "]\n",
        "\n",
        "print(\"Count of downloaded files: \", len(downloaded_files))\n",
        "\n",
        "# Calculate and print the count of files that differ from the expected images in the dataset\n",
        "all_image_filenames = {image['file_name'] for image in data.get('images', [])}\n",
        "different_files = set(downloaded_files) - all_image_filenames\n",
        "print(\"Count of different files: \", len(different_files))\n",
        "\n",
        "# Calculate and print the count of files that differ from the expected images in the dataset\n",
        "all_image_filenames = {image['file_name'] for image in data.get('images', [])}\n",
        "different_files = set(downloaded_files) - all_image_filenames\n",
        "print(\"Count of different files: \", len(different_files))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ML311",
      "language": "python",
      "name": "ml311"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
