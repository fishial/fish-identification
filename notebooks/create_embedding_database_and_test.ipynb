{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Embedding Database Creation & k-NN Classification Testing\n",
        "\n",
        "This notebook:\n",
        "1. Loads BEiT v2 model from checkpoint\n",
        "2. Extracts embeddings from train dataset\n",
        "3. Computes class centroids\n",
        "4. Selects top-100 samples per class for k-NN database\n",
        "5. Tests: ArcFace only vs ArcFace + k-NN on validation set\n",
        "6. Analyzes improvements and failure cases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/home/fishial/Fishial/FishialGithubRepo/fish-identification')\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "from collections import defaultdict, Counter\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, \n",
        "    classification_report, \n",
        "    confusion_matrix,\n",
        "    top_k_accuracy_score\n",
        ")\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import faiss\n",
        "import fiftyone as fo\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# Import your training modules\n",
        "from module.classification_package.src.lightning_trainer_fixed import ImageEmbeddingTrainerViT\n",
        "from module.classification_package.src.datamodule import ImageEmbeddingDataModule\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "def read_json(path):\n",
        "    with open(path, \"r\") as f:\n",
        "        return json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    'checkpoint_path': '/home/fishial/Fishial/Experiments/v10/beitv2_base_patch16_224.in1k_ft_in22k_in1k_20260127_073527/checkpoints/model-epoch=58-val/accuracy_epoch=0.9498.ckpt',\n",
        "    'dataset_name': 'classification_v0.10_train',  # Dataset without train/val tags\n",
        "    'output_dir': '/home/fishial/Fishial/Experiments/v10/embedding_database/beitv2_base_patch16_224.in1k_ft_in22k_in1k_20260127_073527',\n",
        "    'exclude_classes': ['unset', 'Thunnus obesus'],\n",
        "    'label_path': '/home/fishial/Fishial/Experiments/v10/beitv2_base_patch16_224.in1k_ft_in22k_in1k_20260127_073527/labels.json',\n",
        "    'coco_path': '/home/fishial/Fishial/dataset/EXPORT_V_0_9/Fishial_Export_Jan_08_2026_04_14_Production_AI_Gen_All_Verified.json',\n",
        "    # Database parameters\n",
        "    'samples_per_class': 100,  # Top-100 most representative samples\n",
        "    'batch_size': 64,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    \n",
        "    # k-NN parameters (for testing)\n",
        "    'topk_centroid': 5,\n",
        "    'topk_neighbors': 10,\n",
        "    'centroid_threshold': 0.7,\n",
        "    'neighbor_threshold': 0.8,\n",
        "    # Exact match handling\n",
        "    'exclude_exact_matches': False,\n",
        "    'exact_match_tolerance': 1e-4,\n",
        "    \n",
        "    # Dataset split (only if dataset has no train/val tags)\n",
        "    'use_full_dataset': True,  # True = load all as train, False = use tags\n",
        "}\n",
        "\n",
        "# Create output directory\n",
        "Path(CONFIG['output_dir']).mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Device: {CONFIG['device']}\")\n",
        "print(f\"Checkpoint: {Path(CONFIG['checkpoint_path']).name}\")\n",
        "print(f\"Dataset: {CONFIG['dataset_name']} (full dataset: {CONFIG['use_full_dataset']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Model & Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model from Lightning checkpoint\n",
        "print(\"Loading model...\")\n",
        "model = ImageEmbeddingTrainerViT.load_from_checkpoint(\n",
        "    CONFIG['checkpoint_path'],\n",
        "    map_location=CONFIG['device']\n",
        ").eval()\n",
        "\n",
        "print(f\"Model loaded!\")\n",
        "print(f\"  Embedding dim: {model.hparams.embedding_dim}\")\n",
        "print(f\"  Num classes: {model.hparams.num_classes}\")\n",
        "\n",
        "coco = read_json(CONFIG['coco_path'])\n",
        "labels_dict = read_json(CONFIG['label_path'])\n",
        "\n",
        "label_to_species_id = {\n",
        "    c['supercategory']: c.get('fishial_extra', {}).get('species_id')\n",
        "    for c in coco.get('categories', [])\n",
        "    if c.get('name') == 'General body shape' and 'fishial_extra' in c\n",
        "}\n",
        "\n",
        "labels_keys = { int(label_id): {\n",
        "    \"label\": labels_dict[label_id],\n",
        "    \"species_id\": label_to_species_id[labels_dict[label_id]]\n",
        "} for label_id in labels_dict}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create label_to_name mapping for conflict analysis\n",
        "# labels_dict format: {label_id (int): class_name (str), ...}\n",
        "# We need: {label_id (int): class_name (str)} - same structure\n",
        "label_to_name = {int(k): v for k, v in labels_dict.items()}\n",
        "\n",
        "print(f\"Created label_to_name mapping with {len(label_to_name)} classes\")\n",
        "print(f\"Example: label 0 = '{label_to_name.get(0, 'N/A')}'\")\n",
        "print(f\"Example: label 1 = '{label_to_name.get(1, 'N/A')}'\")\n",
        "print(f\"Example: label 2 = '{label_to_name.get(2, 'N/A')}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "print(\"Loading dataset...\")\n",
        "datamodule = ImageEmbeddingDataModule(\n",
        "    dataset_name=CONFIG['dataset_name'],\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    classes_per_batch=32,  # Not used for inference, but required by DataModule\n",
        "    samples_per_class=6,   # Not used for inference, but required by DataModule\n",
        "    image_size=224,\n",
        "    exclude_classes=CONFIG['exclude_classes'],\n",
        "    augmentation_preset='basic',  # Use 'basic'\n",
        "    train_tag=None,  # None = load all samples into train\n",
        "    val_tag=\"val\",    # None = no validation split\n",
        "    class_mapping_path=CONFIG['label_path'],\n",
        "    num_workers=4,\n",
        ")\n",
        "datamodule.setup('fit')\n",
        "\n",
        "print(f\"Dataset loaded!\")\n",
        "print(f\"  Train samples: {len(datamodule.train_dataset)}\")\n",
        "if datamodule.val_dataset is not None:\n",
        "    print(f\"  Val samples: {len(datamodule.val_dataset)}\")\n",
        "else:\n",
        "    print(f\"  Val samples: 0 (no validation split)\")\n",
        "print(f\"  Num classes: {len(labels_keys)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Extract Embeddings from Train Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_sequential_dataloader(dataset, batch_size, num_workers=4):\n",
        "    \"\"\"Create a sequential (non-shuffled) DataLoader for embedding extraction.\"\"\"\n",
        "    from torch.utils.data import DataLoader\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,  # Important: sequential order\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "def extract_embeddings(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Extract normalized embeddings and ArcFace logits from model.\n",
        "    \"\"\"\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "    logits_list = []\n",
        "    image_ids = []\n",
        "    annotation_ids = []\n",
        "    drawn_fish_ids = []\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc='Extracting embeddings'):\n",
        "            images, targets, _, metadata = batch\n",
        "            images = images.to(device)\n",
        "            \n",
        "            # Get embeddings and logits\n",
        "            emb_norm, logits, _ = model(images)\n",
        "            \n",
        "            embeddings.append(emb_norm.cpu())\n",
        "            logits_list.append(logits.cpu())\n",
        "            labels.extend(targets.cpu().numpy())\n",
        "            \n",
        "            # Metadata\n",
        "            if 'image_id' in metadata:\n",
        "                image_ids.extend(metadata['image_id'])\n",
        "            if 'annotation_id' in metadata:\n",
        "                annotation_ids.extend(metadata['annotation_id'])\n",
        "            if 'drawn_fish_id' in metadata:\n",
        "                drawn_fish_ids.extend(metadata['drawn_fish_id'])\n",
        "    \n",
        "    embeddings = torch.cat(embeddings, dim=0).numpy()\n",
        "    logits = torch.cat(logits_list, dim=0).numpy()\n",
        "    labels = np.array(labels)\n",
        "    \n",
        "    return {\n",
        "        'embeddings': embeddings,\n",
        "        'logits': logits,\n",
        "        'labels': labels,\n",
        "        'image_ids': image_ids if image_ids else None,\n",
        "        'annotation_ids': annotation_ids if annotation_ids else None,\n",
        "        'drawn_fish_ids': drawn_fish_ids if drawn_fish_ids else None,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract train embeddings\n",
        "# Note: We use a sequential dataloader (not MPerClassSampler) to preserve order\n",
        "print(\"Extracting train embeddings...\")\n",
        "train_dataloader = create_sequential_dataloader(\n",
        "    datamodule.train_dataset, \n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    num_workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "train_data = extract_embeddings(\n",
        "    model, \n",
        "    train_dataloader, \n",
        "    CONFIG['device']\n",
        ")\n",
        "\n",
        "print(f\"Extracted {len(train_data['embeddings'])} train embeddings\")\n",
        "print(f\"Embedding shape: {train_data['embeddings'].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract validation embeddings (if available)\n",
        "if datamodule.val_dataset is not None:\n",
        "    print(\"Extracting validation embeddings...\")\n",
        "    val_data = extract_embeddings(\n",
        "        model, \n",
        "        datamodule.val_dataloader(), \n",
        "        CONFIG['device']\n",
        "    )\n",
        "    print(f\"Extracted {len(val_data['embeddings'])} validation embeddings\")\n",
        "else:\n",
        "    print(\"âš ï¸ No validation split found.\")\n",
        "    print(\"For testing, we'll use a random 10% subset of train data.\")\n",
        "    \n",
        "    # Split train data for testing\n",
        "    n_val = len(train_data['embeddings']) // 10\n",
        "    indices = np.random.RandomState(42).permutation(len(train_data['embeddings']))\n",
        "    val_indices = indices[:n_val]\n",
        "    \n",
        "    val_data = {\n",
        "        'embeddings': train_data['embeddings'][val_indices],\n",
        "        'logits': train_data['logits'][val_indices],\n",
        "        'labels': train_data['labels'][val_indices],\n",
        "        'image_ids': [train_data['image_ids'][i] for i in val_indices] if train_data['image_ids'] else None,\n",
        "        'annotation_ids': [train_data['annotation_ids'][i] for i in val_indices] if train_data['annotation_ids'] else None,\n",
        "        'drawn_fish_ids': [train_data['drawn_fish_ids'][i] for i in val_indices] if train_data['drawn_fish_ids'] else None,\n",
        "    }\n",
        "    print(f\"Created validation subset: {len(val_data['embeddings'])} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Compute Class Centroids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_centroids(embeddings, labels):\n",
        "    \"\"\"\n",
        "    Compute normalized centroid for each class.\n",
        "    \"\"\"\n",
        "    unique_labels = np.unique(labels)\n",
        "    centroids = {}\n",
        "    \n",
        "    for label in tqdm(unique_labels, desc='Computing centroids'):\n",
        "        class_embeddings = embeddings[labels == label]\n",
        "        centroid = np.mean(class_embeddings, axis=0)\n",
        "        # Normalize\n",
        "        centroid /= (np.linalg.norm(centroid) + 1e-10)\n",
        "        centroids[label] = centroid\n",
        "    \n",
        "    return centroids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute centroids\n",
        "centroids = compute_centroids(train_data['embeddings'], train_data['labels'])\n",
        "print(f\"Computed {len(centroids)} class centroids\")\n",
        "\n",
        "# Convert to matrix for efficient computation\n",
        "centroid_labels = list(centroids.keys())\n",
        "centroid_matrix = np.stack([centroids[label] for label in centroid_labels])\n",
        "print(f\"Centroid matrix shape: {centroid_matrix.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Select Top-100 Representative Samples per Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def select_representative_samples(embeddings, labels, metadata, centroids, samples_per_class=100):\n",
        "#     \"\"\"\n",
        "#     Select top-N most representative samples per class (closest to centroid).\n",
        "    \n",
        "#     âš ï¸ DEPRECATED: This is the old version without conflict filtering.\n",
        "#     Use select_representative_samples_v2() instead for better results.\n",
        "#     \"\"\"\n",
        "#     selected_indices = []\n",
        "#     unique_labels = np.unique(labels)\n",
        "    \n",
        "#     for label in tqdm(unique_labels, desc='Selecting samples'):\n",
        "#         class_mask = labels == label\n",
        "#         class_embeddings = embeddings[class_mask]\n",
        "#         class_indices = np.where(class_mask)[0]\n",
        "        \n",
        "#         # Compute distances to centroid\n",
        "#         centroid = centroids[label]\n",
        "#         distances = 1.0 - np.dot(class_embeddings, centroid)  # Cosine distance\n",
        "        \n",
        "#         # Select top-N closest to centroid\n",
        "#         n_select = min(samples_per_class, len(class_indices))\n",
        "#         top_n_local_indices = np.argsort(distances)[:n_select]\n",
        "#         top_n_global_indices = class_indices[top_n_local_indices]\n",
        "        \n",
        "#         selected_indices.extend(top_n_global_indices)\n",
        "    \n",
        "#     selected_indices = np.array(selected_indices)\n",
        "    \n",
        "#     # Create filtered database\n",
        "#     db = {\n",
        "#         'embeddings': embeddings[selected_indices],\n",
        "#         'labels': labels[selected_indices],\n",
        "#     }\n",
        "    \n",
        "#     # Add metadata if available\n",
        "#     for key in ['image_ids', 'annotation_ids', 'drawn_fish_ids']:\n",
        "#         if key in metadata and metadata[key] is not None:\n",
        "#             db[key] = [metadata[key][i] for i in selected_indices]\n",
        "    \n",
        "#     return db, selected_indices\n",
        "\n",
        "def filter_conflicting_embeddings(embeddings, labels, metadata, \n",
        "                                   similarity_threshold=0.8, \n",
        "                                   min_neighbors_check=5,\n",
        "                                   verbose=True):\n",
        "    \"\"\"\n",
        "    Filter out embeddings that are too similar to samples from other classes.\n",
        "    \n",
        "    This helps remove potential labeling errors where identical/similar images \n",
        "    are assigned to different classes.\n",
        "    \n",
        "    Args:\n",
        "        embeddings: numpy array of embeddings (N, D)\n",
        "        labels: numpy array of labels (N,)\n",
        "        metadata: dict with 'image_ids', 'annotation_ids', 'drawn_fish_ids'\n",
        "        similarity_threshold: cosine similarity threshold (0.95 = very similar)\n",
        "        min_neighbors_check: how many nearest neighbors to check\n",
        "        verbose: print statistics\n",
        "    \n",
        "    Returns:\n",
        "        Filtered embeddings, labels, metadata, and list of removed indices\n",
        "    \"\"\"\n",
        "    N = len(embeddings)\n",
        "    \n",
        "    # Build FAISS index for fast similarity search\n",
        "    if verbose:\n",
        "        print(f\"Building FAISS index for {N} embeddings...\")\n",
        "    \n",
        "    # Normalize embeddings for cosine similarity\n",
        "    normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "    \n",
        "    # Create FAISS index\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dimension)  # Inner Product = cosine similarity (after normalization)\n",
        "    index.add(normalized_embeddings.astype('float32'))\n",
        "    \n",
        "    # Find conflicting samples\n",
        "    if verbose:\n",
        "        print(f\"Searching for conflicting embeddings (similarity > {similarity_threshold})...\")\n",
        "    \n",
        "    k = min_neighbors_check + 1  # +1 because first neighbor is itself\n",
        "    similarities, indices = index.search(normalized_embeddings.astype('float32'), k)\n",
        "    \n",
        "    conflicting_indices = set()\n",
        "    conflict_details = []\n",
        "    \n",
        "    for i in tqdm(range(N), desc='Checking conflicts', disable=not verbose):\n",
        "        my_label = labels[i]\n",
        "        \n",
        "        # Check neighbors (skip first one - it's the sample itself)\n",
        "        for j_local, (neighbor_idx, sim) in enumerate(zip(indices[i][1:], similarities[i][1:]), 1):\n",
        "            neighbor_label = labels[neighbor_idx]\n",
        "            \n",
        "            # If very similar but different class - mark as conflict\n",
        "            if sim >= similarity_threshold and neighbor_label != my_label:\n",
        "                conflicting_indices.add(i)\n",
        "                conflicting_indices.add(neighbor_idx)\n",
        "                \n",
        "                conflict_details.append({\n",
        "                    'idx1': i,\n",
        "                    'idx2': neighbor_idx,\n",
        "                    'label1': my_label,\n",
        "                    'label2': neighbor_label,\n",
        "                    'similarity': sim,\n",
        "                    'image_id1': metadata['image_ids'][i] if 'image_ids' in metadata else None,\n",
        "                    'image_id2': metadata['image_ids'][neighbor_idx] if 'image_ids' in metadata else None,\n",
        "                    'ann_id1': metadata['annotation_ids'][i] if 'annotation_ids' in metadata else None,\n",
        "                    'ann_id2': metadata['annotation_ids'][neighbor_idx] if 'annotation_ids' in metadata else None,\n",
        "                })\n",
        "                \n",
        "                if verbose and len(conflict_details) <= 10:  # Print first 10\n",
        "                    print(f\"  Conflict: idx {i} (label {my_label}) <-> idx {neighbor_idx} (label {neighbor_label}), sim={sim:.4f}\")\n",
        "    \n",
        "    # Create mask for non-conflicting samples\n",
        "    keep_mask = np.ones(N, dtype=bool)\n",
        "    keep_mask[list(conflicting_indices)] = False\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\nConflict Statistics:\")\n",
        "        print(f\"  Total samples: {N}\")\n",
        "        print(f\"  Conflicting samples: {len(conflicting_indices)} ({len(conflicting_indices)/N*100:.2f}%)\")\n",
        "        print(f\"  Remaining samples: {keep_mask.sum()} ({keep_mask.sum()/N*100:.2f}%)\")\n",
        "        print(f\"  Unique conflict pairs: {len(conflict_details)}\")\n",
        "    \n",
        "    # Filter data\n",
        "    filtered_embeddings = embeddings[keep_mask]\n",
        "    filtered_labels = labels[keep_mask]\n",
        "    \n",
        "    filtered_metadata = {}\n",
        "    for key in ['image_ids', 'annotation_ids', 'drawn_fish_ids']:\n",
        "        if key in metadata and metadata[key] is not None:\n",
        "            if isinstance(metadata[key], np.ndarray):\n",
        "                filtered_metadata[key] = metadata[key][keep_mask]\n",
        "            elif isinstance(metadata[key], list):\n",
        "                filtered_metadata[key] = [metadata[key][i] for i in range(N) if keep_mask[i]]\n",
        "    \n",
        "    removed_indices = np.where(~keep_mask)[0]\n",
        "    \n",
        "    return {\n",
        "        'embeddings': filtered_embeddings,\n",
        "        'labels': filtered_labels,\n",
        "        'metadata': filtered_metadata,\n",
        "        'removed_indices': removed_indices,\n",
        "        'conflict_details': conflict_details,\n",
        "        'n_original': N,\n",
        "        'n_filtered': keep_mask.sum()\n",
        "    }\n",
        "\n",
        "def select_representative_samples_v2(embeddings, labels, metadata, centroids, \n",
        "                                      samples_per_class=100,\n",
        "                                      filter_conflicts=True,\n",
        "                                      conflict_similarity_threshold=0.95,\n",
        "                                      conflict_neighbors_check=5):\n",
        "    \"\"\"\n",
        "    Select top-N most representative samples per class (closest to centroid).\n",
        "    \n",
        "    Now with optional conflict filtering to remove potentially mislabeled samples.\n",
        "    \n",
        "    Args:\n",
        "        embeddings: numpy array of embeddings (N, D)\n",
        "        labels: numpy array of labels (N,)\n",
        "        metadata: dict with 'image_ids', 'annotation_ids', 'drawn_fish_ids'\n",
        "        centroids: numpy array of class centroids (num_classes, D)\n",
        "        samples_per_class: number of samples to select per class\n",
        "        filter_conflicts: whether to filter conflicting embeddings first\n",
        "        conflict_similarity_threshold: similarity threshold for conflict detection\n",
        "        conflict_neighbors_check: number of neighbors to check for conflicts\n",
        "    \n",
        "    Returns:\n",
        "        db: dict with filtered embeddings and labels\n",
        "        selected_indices: global indices of selected samples (before conflict filtering)\n",
        "        filter_info: dict with conflict filtering statistics (if filter_conflicts=True)\n",
        "    \"\"\"\n",
        "    filter_info = None\n",
        "    \n",
        "    # Step 1: Filter conflicts if requested\n",
        "    if filter_conflicts:\n",
        "        print(\"=\" * 60)\n",
        "        print(\"STEP 1: Filtering conflicting embeddings\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        filter_result = filter_conflicting_embeddings(\n",
        "            embeddings, \n",
        "            labels, \n",
        "            metadata,\n",
        "            similarity_threshold=conflict_similarity_threshold,\n",
        "            min_neighbors_check=conflict_neighbors_check,\n",
        "            verbose=True\n",
        "        )\n",
        "        \n",
        "        embeddings = filter_result['embeddings']\n",
        "        labels = filter_result['labels']\n",
        "        metadata = filter_result['metadata']\n",
        "        filter_info = {\n",
        "            'removed_indices': filter_result['removed_indices'],\n",
        "            'conflict_details': filter_result['conflict_details'],\n",
        "            'n_original': filter_result['n_original'],\n",
        "            'n_filtered': filter_result['n_filtered']\n",
        "        }\n",
        "        \n",
        "        print(f\"\\nâœ“ Filtered from {filter_result['n_original']} to {filter_result['n_filtered']} samples\")\n",
        "        print()\n",
        "    \n",
        "    # Step 2: Select representative samples\n",
        "    print(\"=\" * 60)\n",
        "    print(\"STEP 2: Selecting representative samples per class\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    selected_indices = []\n",
        "    unique_labels = np.unique(labels)\n",
        "    \n",
        "    for label in tqdm(unique_labels, desc='Selecting samples'):\n",
        "        class_mask = labels == label\n",
        "        class_embeddings = embeddings[class_mask]\n",
        "        class_indices = np.where(class_mask)[0]\n",
        "        \n",
        "        # Compute distances to centroid\n",
        "        centroid = centroids[label]\n",
        "        distances = 1.0 - np.dot(class_embeddings, centroid)  # Cosine distance\n",
        "        \n",
        "        # Select top-N closest to centroid\n",
        "        n_select = min(samples_per_class, len(class_indices))\n",
        "        top_n_local_indices = np.argsort(distances)[:n_select]\n",
        "        top_n_global_indices = class_indices[top_n_local_indices]\n",
        "        \n",
        "        selected_indices.extend(top_n_global_indices)\n",
        "    \n",
        "    selected_indices = np.array(selected_indices)\n",
        "    \n",
        "    # Create filtered database\n",
        "    db = {\n",
        "        'embeddings': embeddings[selected_indices],\n",
        "        'labels': labels[selected_indices],\n",
        "    }\n",
        "    \n",
        "    # Add metadata if available\n",
        "    for key in ['image_ids', 'annotation_ids', 'drawn_fish_ids']:\n",
        "        if key in metadata and metadata[key] is not None:\n",
        "            if isinstance(metadata[key], np.ndarray):\n",
        "                db[key] = metadata[key][selected_indices]\n",
        "            elif isinstance(metadata[key], list):\n",
        "                db[key] = [metadata[key][i] for i in selected_indices]\n",
        "    \n",
        "    print(f\"\\nâœ“ Selected {len(db['embeddings'])} representative samples\")\n",
        "    \n",
        "    return db, selected_indices, filter_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply conflict filtering and select representative samples\n",
        "database, selected_indices, filter_info = select_representative_samples_v2(\n",
        "    train_data['embeddings'],\n",
        "    train_data['labels'],\n",
        "    {\n",
        "        'image_ids': train_data['image_ids'],\n",
        "        'annotation_ids': train_data['annotation_ids'],\n",
        "        'drawn_fish_ids': train_data['drawn_fish_ids'],\n",
        "    },\n",
        "    centroids,\n",
        "    samples_per_class=CONFIG['samples_per_class'],\n",
        "    filter_conflicts=True,  # Enable conflict filtering\n",
        "    conflict_similarity_threshold=0.99,  # Cosine similarity threshold (0.95 = very similar)\n",
        "    conflict_neighbors_check=5  # Check 5 nearest neighbors\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FINAL DATABASE STATISTICS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Database embedding shape: {database['embeddings'].shape}\")\n",
        "print(f\"Total samples in database: {len(database['embeddings'])}\")\n",
        "\n",
        "# Per-class statistics\n",
        "label_counts = Counter(database['labels'])\n",
        "print(f\"\\nSamples per class distribution:\")\n",
        "print(f\"  Min: {min(label_counts.values())}\")\n",
        "print(f\"  Max: {max(label_counts.values())}\")\n",
        "print(f\"  Mean: {np.mean(list(label_counts.values())):.1f}\")\n",
        "print(f\"  Median: {np.median(list(label_counts.values())):.1f}\")\n",
        "\n",
        "if filter_info:\n",
        "    print(f\"\\nConflict Filtering Impact:\")\n",
        "    print(f\"  Original samples: {filter_info['n_original']}\")\n",
        "    print(f\"  After filtering: {filter_info['n_filtered']}\")\n",
        "    print(f\"  Removed: {len(filter_info['removed_indices'])} ({len(filter_info['removed_indices'])/filter_info['n_original']*100:.2f}%)\")\n",
        "    print(f\"  Conflict pairs found: {len(filter_info['conflict_details'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save database\n",
        "database_path = Path(CONFIG['output_dir']) / 'embedding_database_beitv2_top100.pt'\n",
        "\n",
        "torch.save({\n",
        "    'embeddings': torch.from_numpy(database['embeddings']),\n",
        "    'labels': database['labels'],\n",
        "    'image_ids': database.get('image_ids'),\n",
        "    'annotation_ids': database.get('annotation_ids'),\n",
        "    'drawn_fish_ids': database.get('drawn_fish_ids'),\n",
        "    'labels_keys': labels_keys,\n",
        "    'centroids': centroids,\n",
        "    'config': CONFIG,\n",
        "}, database_path)\n",
        "\n",
        "print(f\"âœ… Database saved to: {database_path}\")\n",
        "print(f\"   Size: {database_path.stat().st_size / 1024 / 1024:.1f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. k-NN Classifier Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EmbeddingKNNClassifier:\n",
        "    \"\"\"\n",
        "    k-NN classifier using embeddings, centroids, and FAISS.\n",
        "    \"\"\"\n",
        "    def __init__(self, database, centroids, config):\n",
        "        self.db_embeddings = database['embeddings']\n",
        "        self.db_labels = database['labels']\n",
        "        self.centroids = centroids\n",
        "        self.config = config\n",
        "        \n",
        "        # Prepare centroid matrix\n",
        "        self.centroid_labels = list(centroids.keys())\n",
        "        self.centroid_matrix = np.stack([centroids[label] for label in self.centroid_labels])\n",
        "        self.exclude_exact_matches = self.config.get('exclude_exact_matches', False)\n",
        "        self.exact_match_tolerance = self.config.get('exact_match_tolerance', 1e-4)\n",
        "        \n",
        "        print(f\"KNN Classifier initialized with {len(self.db_embeddings)} samples\")\n",
        "    \n",
        "    def predict(self, query_embeddings, return_details=False):\n",
        "        \"\"\"\n",
        "        Predict using centroid filtering + k-NN search.\n",
        "        \"\"\"\n",
        "        if isinstance(query_embeddings, torch.Tensor):\n",
        "            query_embeddings = query_embeddings.cpu().numpy()\n",
        "        \n",
        "        predictions = []\n",
        "        details = []\n",
        "\n",
        "        \n",
        "        for query_emb in query_embeddings:\n",
        "            # Step 1: Find top-K centroids\n",
        "            centroid_sims = 1.0 - pairwise_distances(\n",
        "                query_emb.reshape(1, -1), \n",
        "                self.centroid_matrix, \n",
        "                metric='cosine'\n",
        "            )[0]\n",
        "            \n",
        "            top_centroid_indices = np.argsort(-centroid_sims)[:self.config['topk_centroid']]\n",
        "            \n",
        "            # Filter by threshold\n",
        "            centroid_scores = {\n",
        "                self.centroid_labels[idx]: centroid_sims[idx]\n",
        "                for idx in top_centroid_indices \n",
        "                if centroid_sims[idx] >= self.config['centroid_threshold']\n",
        "            }\n",
        "            \n",
        "            if not centroid_scores:\n",
        "                # Fallback: use top-1 centroid\n",
        "                best_idx = np.argmax(centroid_sims)\n",
        "                predictions.append(self.centroid_labels[best_idx])\n",
        "                if return_details:\n",
        "                    details.append({'centroid_scores': {}, 'neighbor_votes': {}})\n",
        "                continue\n",
        "            \n",
        "            selected_classes = set(centroid_scores.keys())\n",
        "            \n",
        "            # Step 2: Filter database by selected classes\n",
        "            class_mask = np.isin(self.db_labels, list(selected_classes))\n",
        "            selected_embeddings = self.db_embeddings[class_mask]\n",
        "            selected_labels = self.db_labels[class_mask]\n",
        "            \n",
        "            if len(selected_embeddings) == 0:\n",
        "                best_class = max(centroid_scores, key=centroid_scores.get)\n",
        "                predictions.append(best_class)\n",
        "                if return_details:\n",
        "                    details.append({'centroid_scores': centroid_scores, 'neighbor_votes': {}})\n",
        "                continue\n",
        "            \n",
        "            # Step 3: k-NN search with FAISS\n",
        "            dim = selected_embeddings.shape[1]\n",
        "            index = faiss.IndexFlatIP(dim)  # Inner product (for normalized vectors = cosine)\n",
        "            index.add(selected_embeddings.astype('float32'))\n",
        "            \n",
        "            k = min(self.config['topk_neighbors'], len(selected_embeddings))\n",
        "            distances, indices = index.search(query_emb.reshape(1, -1).astype('float32'), k)\n",
        "            \n",
        "            # Step 4: Vote from neighbors\n",
        "            neighbor_votes = defaultdict(lambda: {'count': 0, 'total_sim': 0.0})\n",
        "            for idx, sim in zip(indices[0], distances[0]):\n",
        "                if sim >= 1.0:\n",
        "                    continue\n",
        "                if sim >= self.config['neighbor_threshold']:\n",
        "                    label = selected_labels[idx]\n",
        "                    neighbor_votes[label]['count'] += 1\n",
        "                    neighbor_votes[label]['total_sim'] += sim\n",
        "            \n",
        "            # Step 5: Combine centroid + neighbor scores\n",
        "            final_scores = {}\n",
        "            for label in selected_classes:\n",
        "                centroid_score = centroid_scores.get(label, 0.0)\n",
        "                neighbor_score = neighbor_votes[label]['total_sim'] if label in neighbor_votes else 0.0\n",
        "                neighbor_count = neighbor_votes[label]['count'] if label in neighbor_votes else 0\n",
        "                \n",
        "                # Weighted combination\n",
        "                final_scores[label] = (\n",
        "                    0.3 * centroid_score + \n",
        "                    0.7 * (neighbor_score / max(neighbor_count, 1))\n",
        "                )\n",
        "            \n",
        "            # Predict\n",
        "            best_label = max(final_scores, key=final_scores.get)\n",
        "            predictions.append(best_label)\n",
        "            \n",
        "            if return_details:\n",
        "                details.append({\n",
        "                    'centroid_scores': centroid_scores,\n",
        "                    'neighbor_votes': dict(neighbor_votes),\n",
        "                    'final_scores': final_scores,\n",
        "                })\n",
        "        \n",
        "        if return_details:\n",
        "            return np.array(predictions), details\n",
        "        return np.array(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize classifier\n",
        "knn_classifier = EmbeddingKNNClassifier(\n",
        "    database=database,\n",
        "    centroids=centroids,\n",
        "    config=CONFIG\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Testing: ArcFace Only vs ArcFace + k-NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method A: ArcFace only (baseline)\n",
        "print(\"Method A: ArcFace head only\")\n",
        "predictions_arcface = np.argmax(val_data['logits'], axis=1)\n",
        "acc_arcface = accuracy_score(val_data['labels'], predictions_arcface)\n",
        "print(f\"  Accuracy: {acc_arcface:.4f} ({acc_arcface*100:.2f}%)\")\n",
        "\n",
        "# Top-2 accuracy\n",
        "top2_arcface = top_k_accuracy_score(\n",
        "    val_data['labels'], \n",
        "    val_data['logits'], \n",
        "    k=2,\n",
        "    labels=np.arange(val_data['logits'].shape[1])\n",
        ")\n",
        "print(f\"  Top-2 Accuracy: {top2_arcface:.4f} ({top2_arcface*100:.2f}%)\")\n",
        "\n",
        "# Top-5 accuracy\n",
        "top5_arcface = top_k_accuracy_score(\n",
        "    val_data['labels'], \n",
        "    val_data['logits'], \n",
        "    k=5,\n",
        "    labels=np.arange(val_data['logits'].shape[1])\n",
        ")\n",
        "print(f\"  Top-5 Accuracy: {top5_arcface:.4f} ({top5_arcface*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method B: ArcFace + k-NN\n",
        "print(\"Method B: ArcFace + k-NN\")\n",
        "predictions_knn, details_knn = knn_classifier.predict(\n",
        "    val_data['embeddings'], \n",
        "    return_details=True\n",
        ")\n",
        "acc_knn = accuracy_score(val_data['labels'], predictions_knn)\n",
        "print(f\"  Accuracy: {acc_knn:.4f} ({acc_knn*100:.2f}%)\")\n",
        "\n",
        "# Top-2 accuracy for k-NN\n",
        "top2_correct = 0\n",
        "for i, (true_label, detail) in enumerate(zip(val_data['labels'], details_knn)):\n",
        "    if 'final_scores' in detail:\n",
        "        # Get top-2 predicted classes\n",
        "        top2_classes = sorted(detail['final_scores'].items(), key=lambda x: x[1], reverse=True)[:2]\n",
        "        top2_labels = [cls for cls, score in top2_classes]\n",
        "        if true_label in top2_labels:\n",
        "            top2_correct += 1\n",
        "    else:\n",
        "        # Fallback: if no final_scores, just check top-1\n",
        "        if predictions_knn[i] == true_label:\n",
        "            top2_correct += 1\n",
        "\n",
        "top2_knn = top2_correct / len(val_data['labels'])\n",
        "print(f\"  Top-2 Accuracy: {top2_knn:.4f} ({top2_knn*100:.2f}%)\")\n",
        "\n",
        "# Improvement\n",
        "improvement = acc_knn - acc_arcface\n",
        "improvement_top2 = top2_knn - top2_arcface\n",
        "print(f\"\\nðŸ“ˆ Improvement (Top-1): {improvement:+.4f} ({improvement*100:+.2f}%)\")\n",
        "print(f\"ðŸ“ˆ Improvement (Top-2): {improvement_top2:+.4f} ({improvement_top2*100:+.2f}%)\")\n",
        "\n",
        "if improvement > 0:\n",
        "    print(f\"âœ… k-NN helps! {improvement*100:.2f}% better\")\n",
        "elif improvement < -0.001:\n",
        "    print(f\"âš ï¸ k-NN hurts! {-improvement*100:.2f}% worse\")\n",
        "else:\n",
        "    print(f\"âž¡ï¸ No significant difference\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Detailed Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-class accuracy comparison\n",
        "def compute_per_class_accuracy(y_true, y_pred, labels_keys):\n",
        "    \"\"\"Compute per-class accuracy.\"\"\"\n",
        "    unique_labels = np.unique(y_true)\n",
        "    results = []\n",
        "    \n",
        "    for label in unique_labels:\n",
        "        mask = y_true == label\n",
        "        if mask.sum() == 0:\n",
        "            continue\n",
        "        \n",
        "        acc = accuracy_score(y_true[mask], y_pred[mask])\n",
        "        count = mask.sum()\n",
        "        \n",
        "        class_name = labels_keys[str(int(label))]['label']\n",
        "        results.append({\n",
        "            'class_id': label,\n",
        "            'class_name': class_name,\n",
        "            'count': count,\n",
        "            'accuracy': acc,\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute per-class accuracy\n",
        "acc_arcface_df = compute_per_class_accuracy(\n",
        "    val_data['labels'], \n",
        "    predictions_arcface, \n",
        "    labels_keys\n",
        ").rename(columns={'accuracy': 'acc_arcface'})\n",
        "\n",
        "acc_knn_df = compute_per_class_accuracy(\n",
        "    val_data['labels'], \n",
        "    predictions_knn, \n",
        "    labels_keys\n",
        ").rename(columns={'accuracy': 'acc_knn'})\n",
        "\n",
        "# Merge\n",
        "comparison_df = acc_arcface_df.merge(\n",
        "    acc_knn_df[['class_id', 'acc_knn']], \n",
        "    on='class_id'\n",
        ")\n",
        "comparison_df['improvement'] = comparison_df['acc_knn'] - comparison_df['acc_arcface']\n",
        "comparison_df = comparison_df.sort_values('improvement', ascending=False)\n",
        "\n",
        "print(\"\\nðŸ“Š Per-class accuracy comparison:\")\n",
        "print(f\"\\nTop 10 classes with MOST improvement (k-NN helps):\")\n",
        "print(comparison_df.head(10)[['class_name', 'count', 'acc_arcface', 'acc_knn', 'improvement']].to_string(index=False))\n",
        "\n",
        "print(f\"\\nTop 10 classes with LEAST improvement (k-NN hurts):\")\n",
        "print(comparison_df.tail(10)[['class_name', 'count', 'acc_arcface', 'acc_knn', 'improvement']].to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization: Improvement distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Histogram of improvements\n",
        "axes[0].hist(comparison_df['improvement'], bins=30, edgecolor='black')\n",
        "axes[0].axvline(0, color='red', linestyle='--', label='No change')\n",
        "axes[0].set_xlabel('Improvement (k-NN - ArcFace)')\n",
        "axes[0].set_ylabel('Number of classes')\n",
        "axes[0].set_title('Distribution of Per-Class Improvements')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Scatter: ArcFace vs k-NN accuracy\n",
        "axes[1].scatter(\n",
        "    comparison_df['acc_arcface'], \n",
        "    comparison_df['acc_knn'],\n",
        "    s=comparison_df['count'],\n",
        "    alpha=0.6,\n",
        "    c=comparison_df['improvement'],\n",
        "    cmap='RdYlGn'\n",
        ")\n",
        "axes[1].plot([0, 1], [0, 1], 'k--', alpha=0.3, label='y=x')\n",
        "axes[1].set_xlabel('ArcFace Accuracy')\n",
        "axes[1].set_ylabel('k-NN Accuracy')\n",
        "axes[1].set_title('Per-Class Accuracy: ArcFace vs k-NN')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].set_xlim([0, 1])\n",
        "axes[1].set_ylim([0, 1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(Path(CONFIG['output_dir']) / 'accuracy_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nðŸ“Š Classes where k-NN helps (improvement > 0.05): {(comparison_df['improvement'] > 0.05).sum()}\")\n",
        "print(f\"ðŸ“Š Classes where k-NN hurts (improvement < -0.05): {(comparison_df['improvement'] < -0.05).sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Analyze Failure Cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find cases where ArcFace was correct but k-NN was wrong\n",
        "arcface_correct = (predictions_arcface == val_data['labels'])\n",
        "knn_wrong = (predictions_knn != val_data['labels'])\n",
        "regression_cases = arcface_correct & knn_wrong\n",
        "\n",
        "print(f\"\\nðŸ” Regression cases (ArcFace âœ… â†’ k-NN âŒ): {regression_cases.sum()}\")\n",
        "\n",
        "if regression_cases.sum() > 0:\n",
        "    regression_indices = np.where(regression_cases)[0][:10]  # Show first 10\n",
        "    \n",
        "    print(\"\\nExample regression cases:\")\n",
        "    for idx in regression_indices:\n",
        "        true_label = str(int(val_data['labels'][idx]))\n",
        "        pred_arcface = predictions_arcface[idx]\n",
        "        pred_knn = str(int(predictions_knn[idx]))\n",
        "        \n",
        "        true_name = labels_keys[true_label]['label']\n",
        "        pred_name = labels_keys[pred_knn]['label']\n",
        "        \n",
        "        print(f\"  Sample {idx}: True={true_name}, ArcFace={true_name} âœ…, k-NN={pred_name} âŒ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find cases where ArcFace was wrong but k-NN was correct\n",
        "arcface_wrong = (predictions_arcface != val_data['labels'])\n",
        "knn_correct = (predictions_knn == val_data['labels'])\n",
        "improvement_cases = arcface_wrong & knn_correct\n",
        "\n",
        "print(f\"\\nâœ… Improvement cases (ArcFace âŒ â†’ k-NN âœ…): {improvement_cases.sum()}\")\n",
        "\n",
        "if improvement_cases.sum() > 0:\n",
        "    improvement_indices = np.where(improvement_cases)[0][:10]  # Show first 10\n",
        "    \n",
        "    print(\"\\nExample improvement cases:\")\n",
        "    for idx in improvement_indices:\n",
        "        true_label =str(int( val_data['labels'][idx]))\n",
        "        pred_arcface = str(int(predictions_arcface[idx]))\n",
        "        pred_knn = predictions_knn[idx]\n",
        "        \n",
        "        true_name = labels_keys[true_label]['label']\n",
        "        pred_arcface_name = labels_keys[pred_arcface]['label']\n",
        "        \n",
        "        print(f\"  Sample {idx}: True={true_name}, ArcFace={pred_arcface_name} âŒ, k-NN={true_name} âœ…\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save comparison dataframe\n",
        "comparison_path = Path(CONFIG['output_dir']) / 'per_class_comparison.csv'\n",
        "comparison_df.to_csv(comparison_path, index=False)\n",
        "print(f\"âœ… Per-class comparison saved to: {comparison_path}\")\n",
        "\n",
        "# Save summary\n",
        "summary = {\n",
        "    'checkpoint': CONFIG['checkpoint_path'],\n",
        "    'database_size': len(database['embeddings']),\n",
        "    'samples_per_class': CONFIG['samples_per_class'],\n",
        "    'val_samples': len(val_data['labels']),\n",
        "    \n",
        "    'accuracy_arcface': float(acc_arcface),\n",
        "    'accuracy_knn': float(acc_knn),\n",
        "    'improvement_absolute': float(improvement),\n",
        "    'improvement_relative': float(improvement / acc_arcface * 100),\n",
        "    \n",
        "    'top5_accuracy_arcface': float(top5_arcface),\n",
        "    \n",
        "    'regression_cases': int(regression_cases.sum()),\n",
        "    'improvement_cases': int(improvement_cases.sum()),\n",
        "    \n",
        "    'config': CONFIG,\n",
        "}\n",
        "\n",
        "summary_path = Path(CONFIG['output_dir']) / 'test_summary.json'\n",
        "with open(summary_path, 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "print(f\"âœ… Test summary saved to: {summary_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Final Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"FINAL SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nðŸ“¦ Database:\")\n",
        "print(f\"   Samples: {len(database['embeddings'])}\")\n",
        "print(f\"   Classes: {len(centroids)}\")\n",
        "print(f\"   Path: {database_path}\")\n",
        "print(f\"\\nðŸ“Š Validation Results:\")\n",
        "print(f\"   Val samples: {len(val_data['labels'])}\")\n",
        "print(f\"   ArcFace only:    {acc_arcface:.4f} ({acc_arcface*100:.2f}%)\")\n",
        "print(f\"   ArcFace + k-NN:  {acc_knn:.4f} ({acc_knn*100:.2f}%)\")\n",
        "print(f\"   Improvement:     {improvement:+.4f} ({improvement*100:+.2f}%)\")\n",
        "print(f\"\\nðŸ“ˆ Analysis:\")\n",
        "print(f\"   Classes improved: {(comparison_df['improvement'] > 0).sum()} / {len(comparison_df)}\")\n",
        "print(f\"   Regression cases: {regression_cases.sum()}\")\n",
        "print(f\"   Improvement cases: {improvement_cases.sum()}\")\n",
        "print(f\"   Net gain: {improvement_cases.sum() - regression_cases.sum():+d} samples\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "if improvement > 0.001:\n",
        "    print(\"\\nâœ… RECOMMENDATION: Use k-NN! It improves accuracy.\")\n",
        "elif improvement < -0.001:\n",
        "    print(\"\\nâš ï¸ WARNING: k-NN hurts accuracy. Stick with ArcFace only.\")\n",
        "    print(\"   Consider adjusting thresholds or using different k.\")\n",
        "else:\n",
        "    print(\"\\nâž¡ï¸ NEUTRAL: No significant difference. Use ArcFace for simplicity.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5b. NEW: Select Representative Samples with Conflict Filtering\n",
        "\n",
        "This improved version:\n",
        "1. **Filters conflicts**: Removes embeddings that are very similar but have different labels (potential labeling errors)\n",
        "2. **Selects representatives**: Chooses top-N samples per class closest to centroids\n",
        "3. **Provides diagnostics**: Shows statistics about removed conflicts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply conflict filtering and select representative samples\n",
        "database_v2, selected_indices_v2, filter_info = select_representative_samples_v2(\n",
        "    train_data['embeddings'],\n",
        "    train_data['labels'],\n",
        "    {\n",
        "        'image_ids': train_data['image_ids'],\n",
        "        'annotation_ids': train_data['annotation_ids'],\n",
        "        'drawn_fish_ids': train_data['drawn_fish_ids'],\n",
        "    },\n",
        "    centroids,\n",
        "    samples_per_class=CONFIG['samples_per_class'],\n",
        "    filter_conflicts=True,  # Enable conflict filtering\n",
        "    conflict_similarity_threshold=0.8,  # Cosine similarity threshold (0.95 = very similar)\n",
        "    conflict_neighbors_check=5  # Check 5 nearest neighbors\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FINAL DATABASE STATISTICS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Database embedding shape: {database_v2['embeddings'].shape}\")\n",
        "print(f\"Total samples in database: {len(database_v2['embeddings'])}\")\n",
        "\n",
        "# Per-class statistics\n",
        "label_counts = Counter(database_v2['labels'])\n",
        "print(f\"\\nSamples per class distribution:\")\n",
        "print(f\"  Min: {min(label_counts.values())}\")\n",
        "print(f\"  Max: {max(label_counts.values())}\")\n",
        "print(f\"  Mean: {np.mean(list(label_counts.values())):.1f}\")\n",
        "print(f\"  Median: {np.median(list(label_counts.values())):.1f}\")\n",
        "\n",
        "if filter_info:\n",
        "    print(f\"\\nConflict Filtering Impact:\")\n",
        "    print(f\"  Original samples: {filter_info['n_original']}\")\n",
        "    print(f\"  After filtering: {filter_info['n_filtered']}\")\n",
        "    print(f\"  Removed: {len(filter_info['removed_indices'])} ({len(filter_info['removed_indices'])/filter_info['n_original']*100:.2f}%)\")\n",
        "    print(f\"  Conflict pairs found: {len(filter_info['conflict_details'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyze Conflict Details\n",
        "\n",
        "Let's investigate which classes have the most conflicts and what are the most common confusion pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if filter_info and len(filter_info['conflict_details']) > 0:\n",
        "    # Check if label_to_name exists, if not create a fallback\n",
        "    if 'label_to_name' not in globals():\n",
        "        print(\"âš ï¸ Warning: label_to_name not found. Using label IDs instead of class names.\")\n",
        "        print(\"   Run the cell that creates label_to_name mapping to see class names.\\n\")\n",
        "        label_to_name = {}\n",
        "    \n",
        "    # Analyze conflict pairs\n",
        "    conflict_pairs = defaultdict(int)\n",
        "    class_conflicts = defaultdict(int)\n",
        "    \n",
        "    for conflict in filter_info['conflict_details']:\n",
        "        label1, label2 = conflict['label1'], conflict['label2']\n",
        "        # Sort to avoid counting (A, B) and (B, A) separately\n",
        "        pair = tuple(sorted([label1, label2]))\n",
        "        conflict_pairs[pair] += 1\n",
        "        class_conflicts[label1] += 1\n",
        "        class_conflicts[label2] += 1\n",
        "    \n",
        "    # Top conflict pairs\n",
        "    print(\"Top 20 Most Common Conflict Pairs:\")\n",
        "    print(\"-\" * 80)\n",
        "    sorted_pairs = sorted(conflict_pairs.items(), key=lambda x: x[1], reverse=True)\n",
        "    for (label1, label2), count in sorted_pairs[:20]:\n",
        "        # Get class names if available (from label_to_name mapping)\n",
        "        name1 = label_to_name.get(label1, f\"Class_{label1}\")\n",
        "        name2 = label_to_name.get(label2, f\"Class_{label2}\")\n",
        "        print(f\"{name1:40s} <-> {name2:40s} : {count:4d} conflicts\")\n",
        "    \n",
        "    # Classes with most conflicts\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Top 20 Classes with Most Conflicts:\")\n",
        "    print(\"-\" * 80)\n",
        "    sorted_classes = sorted(class_conflicts.items(), key=lambda x: x[1], reverse=True)\n",
        "    for label, count in sorted_classes[:20]:\n",
        "        name = label_to_name.get(label, f\"Class_{label}\")\n",
        "        print(f\"{name:60s} : {count:4d} conflicts\")\n",
        "    \n",
        "    # Similarity distribution\n",
        "    similarities = [c['similarity'] for c in filter_info['conflict_details']]\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Conflict Similarity Distribution:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"  Min similarity: {min(similarities):.4f}\")\n",
        "    print(f\"  Max similarity: {max(similarities):.4f}\")\n",
        "    print(f\"  Mean similarity: {np.mean(similarities):.4f}\")\n",
        "    print(f\"  Median similarity: {np.median(similarities):.4f}\")\n",
        "    \n",
        "    # Plot similarity distribution\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.hist(similarities, bins=50, edgecolor='black', alpha=0.7)\n",
        "    plt.axvline(np.mean(similarities), color='red', linestyle='--', label=f'Mean: {np.mean(similarities):.4f}')\n",
        "    plt.xlabel('Cosine Similarity')\n",
        "    plt.ylabel('Number of Conflicts')\n",
        "    plt.title('Distribution of Similarity Scores for Conflicting Embeddings')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "else:\n",
        "    print(\"No conflicts found or conflict filtering was not enabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save Conflict Report\n",
        "\n",
        "Save detailed conflict information for manual inspection and potential dataset cleaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if filter_info and len(filter_info['conflict_details']) > 0:\n",
        "    # Check if label_to_name exists, if not create a fallback\n",
        "    if 'label_to_name' not in globals():\n",
        "        print(\"âš ï¸ Warning: label_to_name not found. Using label IDs instead of class names.\")\n",
        "        print(\"   Run the cell that creates label_to_name mapping to see class names.\\n\")\n",
        "        label_to_name = {}\n",
        "    \n",
        "    # Prepare conflict report\n",
        "    conflict_report = []\n",
        "    \n",
        "    for conflict in filter_info['conflict_details']:\n",
        "        # Add class names\n",
        "        conflict_with_names = conflict.copy()\n",
        "        conflict_with_names['class_name1'] = label_to_name.get(conflict['label1'], f\"Class_{conflict['label1']}\")\n",
        "        conflict_with_names['class_name2'] = label_to_name.get(conflict['label2'], f\"Class_{conflict['label2']}\")\n",
        "        conflict_report.append(conflict_with_names)\n",
        "    \n",
        "    # Convert to DataFrame for better inspection\n",
        "    conflict_df = pd.DataFrame(conflict_report)\n",
        "    \n",
        "    # Sort by similarity (highest first)\n",
        "    conflict_df = conflict_df.sort_values('similarity', ascending=False)\n",
        "    \n",
        "    # Save to CSV\n",
        "    conflict_csv_path = Path(CONFIG['output_dir']) / 'conflict_report.csv'\n",
        "    conflict_df.to_csv(conflict_csv_path, index=False)\n",
        "    print(f\"âœ“ Saved conflict report to: {conflict_csv_path}\")\n",
        "    \n",
        "    # Save summary statistics\n",
        "    summary = {\n",
        "        'total_original_samples': filter_info['n_original'],\n",
        "        'total_filtered_samples': filter_info['n_filtered'],\n",
        "        'total_removed_samples': len(filter_info['removed_indices']),\n",
        "        'removal_percentage': len(filter_info['removed_indices']) / filter_info['n_original'] * 100,\n",
        "        'total_conflict_pairs': len(filter_info['conflict_details']),\n",
        "        'similarity_threshold': 0.95,\n",
        "        'neighbors_checked': 5,\n",
        "        'mean_conflict_similarity': float(np.mean([c['similarity'] for c in filter_info['conflict_details']])),\n",
        "        'max_conflict_similarity': float(np.max([c['similarity'] for c in filter_info['conflict_details']])),\n",
        "    }\n",
        "    \n",
        "    summary_path = Path(CONFIG['output_dir']) / 'conflict_summary.json'\n",
        "    with open(summary_path, 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "    print(f\"âœ“ Saved conflict summary to: {summary_path}\")\n",
        "    \n",
        "    # Display first few conflicts\n",
        "    print(\"\\nTop 10 Most Similar Conflicts:\")\n",
        "    print(\"-\" * 120)\n",
        "    display_cols = ['class_name1', 'class_name2', 'similarity', 'image_id1', 'image_id2', 'ann_id1', 'ann_id2']\n",
        "    print(conflict_df[display_cols].head(10).to_string(index=False))\n",
        "    \n",
        "else:\n",
        "    print(\"No conflicts to save.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ“ Parameter Tuning Guide\n",
        "\n",
        "**Conflict Filtering Parameters:**\n",
        "\n",
        "1. **`conflict_similarity_threshold`** (default: 0.95)\n",
        "   - Range: 0.0 to 1.0 (cosine similarity)\n",
        "   - Higher = stricter (only removes very similar conflicts)\n",
        "   - Lower = more aggressive (removes more samples)\n",
        "   - **Recommended values:**\n",
        "     - `0.99`: Very conservative - only identical/near-identical images\n",
        "     - `0.95`: Balanced - catches most labeling errors (RECOMMENDED)\n",
        "     - `0.90`: Aggressive - may remove valid samples\n",
        "     - `0.85`: Very aggressive - use with caution\n",
        "\n",
        "2. **`conflict_neighbors_check`** (default: 5)\n",
        "   - How many nearest neighbors to check for each sample\n",
        "   - Higher = more thorough but slower\n",
        "   - **Recommended values:**\n",
        "     - `3-5`: Fast, catches obvious conflicts (RECOMMENDED)\n",
        "     - `10-20`: More thorough, slower\n",
        "     - `50+`: Very thorough, much slower\n",
        "\n",
        "**When to adjust:**\n",
        "- **High conflict rate (>5%)**: Increase threshold to 0.97-0.99\n",
        "- **Low conflict rate (<0.5%)**: Decrease threshold to 0.90-0.93\n",
        "- **Want more aggressive cleaning**: Decrease threshold, increase neighbors\n",
        "- **Want conservative cleaning**: Increase threshold, decrease neighbors\n",
        "\n",
        "**Tips:**\n",
        "- Start with default values (0.95, 5)\n",
        "- Check conflict report to see actual similarity distributions\n",
        "- Manually inspect top conflicts to verify they are real errors\n",
        "- Adjust threshold based on your data quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸŽ¯ Quick Start Summary\n",
        "\n",
        "**New Workflow with Conflict Filtering:**\n",
        "\n",
        "```python\n",
        "# 1. Apply conflict filtering + select representatives (all-in-one)\n",
        "database_v2, selected_indices_v2, filter_info = select_representative_samples_v2(\n",
        "    train_data['embeddings'],\n",
        "    train_data['labels'],\n",
        "    {'image_ids': train_data['image_ids'], ...},\n",
        "    centroids,\n",
        "    samples_per_class=100,\n",
        "    filter_conflicts=True,              # Enable filtering\n",
        "    conflict_similarity_threshold=0.95,  # Adjust based on data\n",
        "    conflict_neighbors_check=5\n",
        ")\n",
        "\n",
        "# 2. Check results\n",
        "print(f\"Final database size: {len(database_v2['embeddings'])}\")\n",
        "print(f\"Conflicts removed: {len(filter_info['removed_indices'])}\")\n",
        "\n",
        "# 3. Analyze and save\n",
        "# - Check conflict_report.csv for detailed conflict pairs\n",
        "# - Check conflict_summary.json for statistics\n",
        "```\n",
        "\n",
        "**Benefits:**\n",
        "- âœ… Removes potentially mislabeled samples\n",
        "- âœ… Improves database quality\n",
        "- âœ… Reduces confusion between similar classes\n",
        "- âœ… Provides detailed diagnostics\n",
        "\n",
        "**Files Generated:**\n",
        "- `conflict_report.csv` - Detailed list of all conflicts with similarity scores\n",
        "- `conflict_summary.json` - Overall statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (Optional) Visualize Conflict Pairs in FiftyOne\n",
        "\n",
        "Use this to manually inspect conflict pairs and decide if they are true labeling errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_conflict_view_in_fiftyone(conflict_details, dataset_name, max_conflicts=50):\n",
        "    \"\"\"\n",
        "    Create a FiftyOne view showing conflicting image pairs side-by-side.\n",
        "    \n",
        "    Args:\n",
        "        conflict_details: list of conflict dicts from filter_info\n",
        "        dataset_name: name of the FiftyOne dataset\n",
        "        max_conflicts: maximum number of conflicts to show\n",
        "    \"\"\"\n",
        "    # Load dataset\n",
        "    dataset = fo.load_dataset(dataset_name)\n",
        "    \n",
        "    print(f\"Dataset: {dataset_name}\")\n",
        "    print(f\"Total samples in dataset: {len(dataset)}\")\n",
        "    \n",
        "    # Collect unique annotation IDs and image IDs from conflicts\n",
        "    conflict_ann_ids = set()\n",
        "    conflict_image_ids = set()\n",
        "    \n",
        "    for conflict in conflict_details[:max_conflicts]:\n",
        "        if conflict.get('ann_id1'):\n",
        "            conflict_ann_ids.add(str(conflict['ann_id1']))\n",
        "        if conflict.get('ann_id2'):\n",
        "            conflict_ann_ids.add(str(conflict['ann_id2']))\n",
        "        if conflict.get('image_id1'):\n",
        "            conflict_image_ids.add(int(conflict['image_id1']))\n",
        "        if conflict.get('image_id2'):\n",
        "            conflict_image_ids.add(int(conflict['image_id2']))\n",
        "    \n",
        "    print(f\"Looking for {len(conflict_ann_ids)} annotation IDs in {len(conflict_image_ids)} images\")\n",
        "    \n",
        "    # Try to find samples - check multiple possible fields\n",
        "    view = None\n",
        "    \n",
        "    # Strategy 1: Try filtering by sample-level annotation_id field\n",
        "    try:\n",
        "        if len(conflict_ann_ids) > 0:\n",
        "            sample = dataset.first()\n",
        "            if sample and hasattr(sample, 'annotation_id'):\n",
        "                # Filter by annotation_id field at sample level\n",
        "                view = dataset.match(fo.ViewField(\"annotation_id\").is_in(list(conflict_ann_ids)))\n",
        "                if len(view) > 0:\n",
        "                    print(f\"âœ“ Using sample field 'annotation_id'\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Strategy 1 (annotation_id) failed: {e}\")\n",
        "    \n",
        "    # Strategy 2: Try filtering by sample-level image_id field (as string)\n",
        "    if view is None or len(view) == 0:\n",
        "        try:\n",
        "            if len(conflict_image_ids) > 0:\n",
        "                sample = dataset.first()\n",
        "                if sample and hasattr(sample, 'image_id'):\n",
        "                    # Convert image_ids to strings\n",
        "                    image_id_strings = [str(img_id) for img_id in conflict_image_ids]\n",
        "                    view = dataset.match(fo.ViewField(\"image_id\").is_in(image_id_strings))\n",
        "                    if len(view) > 0:\n",
        "                        print(f\"âœ“ Using sample field 'image_id' (as string)\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Strategy 2 (image_id string) failed: {e}\")\n",
        "    \n",
        "    # Strategy 3: Try filtering by detections.id (annotation IDs stored in detections)\n",
        "    if view is None or len(view) == 0:\n",
        "        try:\n",
        "            if len(conflict_ann_ids) > 0:\n",
        "                # Check if dataset has detections with id field\n",
        "                sample = dataset.first()\n",
        "                if sample and hasattr(sample, 'detections') and sample.detections:\n",
        "                    detection_field = 'detections'\n",
        "                    if hasattr(sample.detections, 'detections') and len(sample.detections.detections) > 0:\n",
        "                        first_det = sample.detections.detections[0]\n",
        "                        if hasattr(first_det, 'id'):\n",
        "                            # Filter by detection IDs\n",
        "                            view = dataset.filter_labels(\n",
        "                                detection_field,\n",
        "                                fo.ViewField(\"id\").is_in(list(conflict_ann_ids))\n",
        "                            )\n",
        "                            print(f\"âœ“ Using detection field '{detection_field}' with annotation IDs\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Strategy 3 (detections.id) failed: {e}\")\n",
        "    \n",
        "    # Strategy 4: Try filtering by image_id in metadata\n",
        "    if view is None or len(view) == 0:\n",
        "        try:\n",
        "            if len(conflict_image_ids) > 0:\n",
        "                # Try metadata.image_id (as both string and int)\n",
        "                for converter in [str, int]:\n",
        "                    try:\n",
        "                        converted_ids = [converter(img_id) for img_id in conflict_image_ids]\n",
        "                        view = dataset.match(fo.ViewField(\"metadata.image_id\").is_in(converted_ids))\n",
        "                        if len(view) > 0:\n",
        "                            print(f\"âœ“ Using metadata.image_id field (as {converter.__name__})\")\n",
        "                            break\n",
        "                    except:\n",
        "                        pass\n",
        "        except Exception as e:\n",
        "            print(f\"  Strategy 4 (metadata.image_id) failed: {e}\")\n",
        "    \n",
        "    # Strategy 5: Try filtering by coco_id\n",
        "    if view is None or len(view) == 0:\n",
        "        try:\n",
        "            if len(conflict_image_ids) > 0:\n",
        "                view = dataset.match(fo.ViewField(\"coco_id\").is_in(list(conflict_image_ids)))\n",
        "                if len(view) > 0:\n",
        "                    print(f\"âœ“ Using coco_id field\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Strategy 5 (coco_id) failed: {e}\")\n",
        "    \n",
        "    # Strategy 4: Check available fields and suggest manual approach\n",
        "    if view is None or len(view) == 0:\n",
        "        print(\"\\nâš ï¸ Could not automatically find samples. Dataset schema:\")\n",
        "        sample = dataset.first()\n",
        "        if sample:\n",
        "            print(f\"  Available sample fields: {list(sample.field_names)}\")\n",
        "            if hasattr(sample, 'metadata'):\n",
        "                print(f\"  Metadata fields: {list(sample.metadata.field_names) if hasattr(sample.metadata, 'field_names') else 'N/A'}\")\n",
        "            if hasattr(sample, 'detections'):\n",
        "                print(f\"  Has detections: Yes\")\n",
        "        \n",
        "        print(\"\\n  Showing first few conflict annotation IDs:\")\n",
        "        for i, ann_id in enumerate(list(conflict_ann_ids)[:5]):\n",
        "            print(f\"    - {ann_id}\")\n",
        "        \n",
        "        print(\"\\n  Please check your dataset structure and modify the filter accordingly.\")\n",
        "        return dataset.limit(0)  # Empty view\n",
        "    \n",
        "    print(f\"\\nâœ“ Found {len(view)} samples with conflicts\")\n",
        "    \n",
        "    # Generate color palette for conflict pairs\n",
        "    import colorsys\n",
        "    def generate_colors(n):\n",
        "        \"\"\"Generate N visually distinct colors in hex format.\"\"\"\n",
        "        colors = []\n",
        "        for i in range(n):\n",
        "            hue = i / n\n",
        "            # Use high saturation and medium lightness for vibrant colors\n",
        "            rgb = colorsys.hsv_to_rgb(hue, 0.8, 0.9)\n",
        "            hex_color = '#{:02x}{:02x}{:02x}'.format(\n",
        "                int(rgb[0] * 255), \n",
        "                int(rgb[1] * 255), \n",
        "                int(rgb[2] * 255)\n",
        "            )\n",
        "            colors.append(hex_color)\n",
        "        return colors\n",
        "    \n",
        "    # Generate colors for all pairs\n",
        "    num_pairs = min(len(conflict_details), max_conflicts)\n",
        "    pair_colors = generate_colors(num_pairs)\n",
        "    \n",
        "    # Build annotation ID to conflict info mapping with unique pair IDs\n",
        "    ann_id_to_conflict = {}\n",
        "    for pair_idx, conflict in enumerate(conflict_details[:max_conflicts]):\n",
        "        ann_id1 = str(conflict.get('ann_id1', ''))\n",
        "        ann_id2 = str(conflict.get('ann_id2', ''))\n",
        "        \n",
        "        # Create unique pair identifier\n",
        "        pair_id = f\"pair_{pair_idx + 1:02d}\"\n",
        "        pair_tag = f\"conflict_{pair_id}\"\n",
        "        pair_color = pair_colors[pair_idx]\n",
        "        \n",
        "        if ann_id1:\n",
        "            ann_id_to_conflict[ann_id1] = {\n",
        "                'pair_ann_id': ann_id2,\n",
        "                'pair_label': conflict.get('class_name2', str(conflict.get('label2', 'Unknown'))),\n",
        "                'similarity': conflict['similarity'],\n",
        "                'own_label': conflict.get('class_name1', str(conflict.get('label1', 'Unknown'))),\n",
        "                'pair_id': pair_id,\n",
        "                'pair_tag': pair_tag,\n",
        "                'pair_number': pair_idx + 1,\n",
        "                'pair_color': pair_color,\n",
        "                'role': 'A'  # First sample in pair\n",
        "            }\n",
        "        if ann_id2:\n",
        "            ann_id_to_conflict[ann_id2] = {\n",
        "                'pair_ann_id': ann_id1,\n",
        "                'pair_label': conflict.get('class_name1', str(conflict.get('label1', 'Unknown'))),\n",
        "                'similarity': conflict['similarity'],\n",
        "                'own_label': conflict.get('class_name2', str(conflict.get('label2', 'Unknown'))),\n",
        "                'pair_id': pair_id,\n",
        "                'pair_tag': pair_tag,\n",
        "                'pair_number': pair_idx + 1,\n",
        "                'pair_color': pair_color,\n",
        "                'role': 'B'  # Second sample in pair\n",
        "            }\n",
        "    \n",
        "    print(f\"âœ“ Created {num_pairs} unique conflict pairs with distinct colors\")\n",
        "    \n",
        "    # Add conflict information to samples/detections\n",
        "    tagged_count = 0\n",
        "    pairs_found = set()\n",
        "    \n",
        "    for sample in view:\n",
        "        modified = False\n",
        "        \n",
        "        # Strategy A: Check if sample has annotation_id field (sample-level)\n",
        "        if hasattr(sample, 'annotation_id'):\n",
        "            ann_id = str(sample.annotation_id) if sample.annotation_id else None\n",
        "            if ann_id and ann_id in ann_id_to_conflict:\n",
        "                info = ann_id_to_conflict[ann_id]\n",
        "                \n",
        "                # Add custom fields to sample\n",
        "                sample['conflict_similarity'] = info['similarity']\n",
        "                sample['conflict_pair_label'] = info['pair_label']\n",
        "                sample['conflict_own_label'] = info['own_label']\n",
        "                sample['conflict_pair_ann_id'] = info['pair_ann_id']\n",
        "                sample['conflict_pair_id'] = info['pair_id']\n",
        "                sample['conflict_pair_number'] = info['pair_number']\n",
        "                sample['conflict_role'] = info['role']\n",
        "                sample['conflict_color'] = info['pair_color']\n",
        "                \n",
        "                # Add unique pair tag\n",
        "                pair_tag = info['pair_tag']\n",
        "                if pair_tag not in sample.tags:\n",
        "                    sample.tags.append(pair_tag)\n",
        "                \n",
        "                # Add general conflict tag\n",
        "                if 'conflict' not in sample.tags:\n",
        "                    sample.tags.append('conflict')\n",
        "                \n",
        "                pairs_found.add(info['pair_id'])\n",
        "                modified = True\n",
        "                tagged_count += 1\n",
        "        \n",
        "        # Strategy B: Check detections (detection-level)\n",
        "        elif hasattr(sample, 'detections') and sample.detections:\n",
        "            for det in sample.detections.detections:\n",
        "                det_id = str(det.id) if hasattr(det, 'id') else None\n",
        "                if det_id and det_id in ann_id_to_conflict:\n",
        "                    info = ann_id_to_conflict[det_id]\n",
        "                    \n",
        "                    # Add custom fields\n",
        "                    det['conflict_similarity'] = info['similarity']\n",
        "                    det['conflict_pair_label'] = info['pair_label']\n",
        "                    det['conflict_pair_id'] = info['pair_id']\n",
        "                    det['conflict_pair_number'] = info['pair_number']\n",
        "                    det['is_conflict'] = True\n",
        "                    \n",
        "                    # Add tags\n",
        "                    pair_tag = info['pair_tag']\n",
        "                    if pair_tag not in det.tags:\n",
        "                        det.tags.append(pair_tag)\n",
        "                    if 'conflict' not in det.tags:\n",
        "                        det.tags.append('conflict')\n",
        "                    \n",
        "                    pairs_found.add(info['pair_id'])\n",
        "                    modified = True\n",
        "                    tagged_count += 1\n",
        "            \n",
        "            # Tag sample if any detection has conflict\n",
        "            if modified and 'has_conflict' not in sample.tags:\n",
        "                sample.tags.append('has_conflict')\n",
        "        \n",
        "        # Save changes\n",
        "        if modified:\n",
        "            sample.save()\n",
        "    \n",
        "    print(f\"âœ“ Tagged {tagged_count} samples/detections with conflict information\")\n",
        "    print(f\"âœ“ Found {len(pairs_found)} conflict pairs in the view\")\n",
        "    print(f\"\\n\" + \"=\" * 80)\n",
        "    print(\"HOW TO VIEW CONFLICTS IN FIFTYONE APP:\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\n1ï¸âƒ£  FILTER BY SPECIFIC PAIR:\")\n",
        "    print(f\"   - Use tag filter: 'conflict_pair_01', 'conflict_pair_02', etc.\")\n",
        "    print(f\"   - Each pair has a unique tag and color!\")\n",
        "    \n",
        "    print(f\"\\n2ï¸âƒ£  VIEW ALL CONFLICTS:\")\n",
        "    print(f\"   - Filter by tag: 'conflict'\")\n",
        "    print(f\"   - Sort by 'conflict_pair_number' to group pairs together\")\n",
        "    \n",
        "    print(f\"\\n3ï¸âƒ£  USEFUL FIELDS:\")\n",
        "    print(f\"   - conflict_pair_id: Unique pair identifier (e.g., 'pair_01')\")\n",
        "    print(f\"   - conflict_pair_number: Numeric pair ID (1, 2, 3, ...)\")\n",
        "    print(f\"   - conflict_own_label: This sample's class\")\n",
        "    print(f\"   - conflict_pair_label: The conflicting sample's class\")\n",
        "    print(f\"   - conflict_similarity: How similar they are (0-1)\")\n",
        "    print(f\"   - conflict_role: 'A' or 'B' (which sample in the pair)\")\n",
        "    print(f\"   - conflict_color: Unique color for visualization\")\n",
        "    \n",
        "    print(f\"\\n4ï¸âƒ£  COLOR CODING:\")\n",
        "    print(f\"   - Each pair has a unique color in 'conflict_color' field\")\n",
        "    print(f\"   - Use this to visually identify pairs\")\n",
        "    \n",
        "    print(f\"\\nðŸ’¡ TIP: Sort by 'conflict_pair_number' to see pairs side-by-side!\")\n",
        "    \n",
        "    return view\n",
        "\n",
        "# Example usage (uncomment to run):\n",
        "# if filter_info and len(filter_info['conflict_details']) > 0:\n",
        "#     conflict_view = create_conflict_view_in_fiftyone(\n",
        "#         filter_info['conflict_details'],\n",
        "#         CONFIG['dataset_name'],\n",
        "#         max_conflicts=50\n",
        "#     )\n",
        "#     \n",
        "#     # Launch FiftyOne App\n",
        "#     session = fo.launch_app(conflict_view)\n",
        "#     print(\"\\nInspect conflicts in FiftyOne App!\")\n",
        "#     print(\"Look at samples with tag:'conflict' and check 'conflict_pair_label' field\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_conflict_pairs_visualization(conflict_view):\n",
        "    \"\"\"\n",
        "    Create a nicely organized view of conflict pairs, sorted by pair number.\n",
        "    \n",
        "    Args:\n",
        "        conflict_view: FiftyOne view with tagged conflicts\n",
        "    \n",
        "    Returns:\n",
        "        Sorted view with pairs grouped together\n",
        "    \"\"\"\n",
        "    # Sort by pair number so pairs appear together\n",
        "    sorted_view = conflict_view.sort_by(\"conflict_pair_number\")\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"CONFLICT PAIRS SUMMARY\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Group by pairs and show summary\n",
        "    pairs_info = {}\n",
        "    for sample in sorted_view:\n",
        "        if hasattr(sample, 'conflict_pair_number'):\n",
        "            pair_num = sample.conflict_pair_number\n",
        "            if pair_num not in pairs_info:\n",
        "                pairs_info[pair_num] = {\n",
        "                    'samples': [],\n",
        "                    'labels': set(),\n",
        "                    'similarity': sample.conflict_similarity if hasattr(sample, 'conflict_similarity') else None,\n",
        "                    'color': sample.conflict_color if hasattr(sample, 'conflict_color') else None\n",
        "                }\n",
        "            pairs_info[pair_num]['samples'].append(sample.id)\n",
        "            if hasattr(sample, 'conflict_own_label'):\n",
        "                pairs_info[pair_num]['labels'].add(sample.conflict_own_label)\n",
        "    \n",
        "    # Print summary\n",
        "    for pair_num in sorted(pairs_info.keys()):\n",
        "        info = pairs_info[pair_num]\n",
        "        labels_str = \" â†”ï¸ \".join(sorted(info['labels']))\n",
        "        print(f\"\\nðŸ“Œ Pair {pair_num:02d} (Tag: conflict_pair_{pair_num:02d})\")\n",
        "        print(f\"   Classes: {labels_str}\")\n",
        "        print(f\"   Similarity: {info['similarity']:.4f}\")\n",
        "        print(f\"   Color: {info['color']}\")\n",
        "        print(f\"   Samples: {len(info['samples'])}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"Total: {len(pairs_info)} pairs, {len(sorted_view)} samples\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    return sorted_view\n",
        "\n",
        "\n",
        "def view_specific_pair(conflict_view, pair_number):\n",
        "    \"\"\"\n",
        "    View a specific conflict pair.\n",
        "    \n",
        "    Args:\n",
        "        conflict_view: FiftyOne view with tagged conflicts\n",
        "        pair_number: Pair number to view (1, 2, 3, ...)\n",
        "    \n",
        "    Returns:\n",
        "        View containing only the specified pair\n",
        "    \"\"\"\n",
        "    pair_tag = f\"conflict_pair_{pair_number:02d}\"\n",
        "    pair_view = conflict_view.match_tags(pair_tag)\n",
        "    \n",
        "    print(f\"=\" * 80)\n",
        "    print(f\"VIEWING CONFLICT PAIR {pair_number}\")\n",
        "    print(f\"=\" * 80)\n",
        "    \n",
        "    if len(pair_view) == 0:\n",
        "        print(f\"âš ï¸ No samples found for pair {pair_number}\")\n",
        "        return pair_view\n",
        "    \n",
        "    # Show pair details\n",
        "    for i, sample in enumerate(pair_view, 1):\n",
        "        print(f\"\\nSample {i}/{len(pair_view)}:\")\n",
        "        print(f\"  ID: {sample.id}\")\n",
        "        if hasattr(sample, 'conflict_own_label'):\n",
        "            print(f\"  Label: {sample.conflict_own_label}\")\n",
        "        if hasattr(sample, 'conflict_pair_label'):\n",
        "            print(f\"  Conflicts with: {sample.conflict_pair_label}\")\n",
        "        if hasattr(sample, 'conflict_similarity'):\n",
        "            print(f\"  Similarity: {sample.conflict_similarity:.4f}\")\n",
        "        if hasattr(sample, 'conflict_role'):\n",
        "            print(f\"  Role in pair: {sample.conflict_role}\")\n",
        "    \n",
        "    return pair_view\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# sorted_view = create_conflict_pairs_visualization(conflict_view)\n",
        "# session = fo.launch_app(sorted_view)\n",
        "#\n",
        "# # Or view specific pair:\n",
        "# pair_1_view = view_specific_pair(conflict_view, 1)\n",
        "# session = fo.launch_app(pair_1_view)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Troubleshooting: If view is empty\n",
        "\n",
        "If the conflict view shows 0 samples, try this diagnostic cell below to understand your dataset structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DIAGNOSTIC: Understand dataset structure and find correct field for filtering\n",
        "if filter_info and len(filter_info['conflict_details']) > 0:\n",
        "    dataset = fo.load_dataset(CONFIG['dataset_name'])\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"DATASET DIAGNOSTIC\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Get first sample\n",
        "    sample = dataset.first()\n",
        "    \n",
        "    print(f\"\\n1. Dataset Info:\")\n",
        "    print(f\"   Name: {dataset.name}\")\n",
        "    print(f\"   Total samples: {len(dataset)}\")\n",
        "    print(f\"   Media type: {dataset.media_type}\")\n",
        "    \n",
        "    print(f\"\\n2. Sample Fields:\")\n",
        "    for field_name in sample.field_names:\n",
        "        field_value = getattr(sample, field_name, None)\n",
        "        field_type = type(field_value).__name__\n",
        "        print(f\"   - {field_name}: {field_type}\")\n",
        "        \n",
        "        # Show first few values for small fields\n",
        "        if field_name in ['id', 'coco_id'] and field_value:\n",
        "            print(f\"     Value: {field_value}\")\n",
        "    \n",
        "    print(f\"\\n3. Metadata Fields:\")\n",
        "    if hasattr(sample, 'metadata') and sample.metadata:\n",
        "        for field_name in sample.metadata.field_names:\n",
        "            field_value = getattr(sample.metadata, field_name, None)\n",
        "            print(f\"   - metadata.{field_name}: {field_value}\")\n",
        "    \n",
        "    print(f\"\\n4. Detections Info:\")\n",
        "    if hasattr(sample, 'detections') and sample.detections:\n",
        "        dets = sample.detections.detections\n",
        "        print(f\"   Number of detections: {len(dets)}\")\n",
        "        if len(dets) > 0:\n",
        "            first_det = dets[0]\n",
        "            print(f\"   Detection fields:\")\n",
        "            for attr in dir(first_det):\n",
        "                if not attr.startswith('_'):\n",
        "                    try:\n",
        "                        val = getattr(first_det, attr)\n",
        "                        if not callable(val):\n",
        "                            print(f\"     - {attr}: {val if attr not in ['label', 'id'] else str(val)[:50]}\")\n",
        "                    except:\n",
        "                        pass\n",
        "    \n",
        "    print(f\"\\n5. Sample Conflict Info:\")\n",
        "    print(f\"   First conflict annotation_id1: {filter_info['conflict_details'][0].get('ann_id1')}\")\n",
        "    print(f\"   First conflict annotation_id2: {filter_info['conflict_details'][0].get('ann_id2')}\")\n",
        "    print(f\"   First conflict image_id1: {filter_info['conflict_details'][0].get('image_id1')}\")\n",
        "    print(f\"   First conflict image_id2: {filter_info['conflict_details'][0].get('image_id2')}\")\n",
        "    \n",
        "    print(f\"\\n6. Suggested Fix:\")\n",
        "    print(f\"   Based on the above, modify create_conflict_view_in_fiftyone() to use the correct field.\")\n",
        "    print(f\"   Common patterns:\")\n",
        "    print(f\"     - If detections have 'id': Filter by detections.id\")\n",
        "    print(f\"     - If samples have 'coco_id': Filter by coco_id matching image_id\")\n",
        "    print(f\"     - If metadata has 'image_id': Filter by metadata.image_id\")\n",
        "    \n",
        "    # Try to find a matching sample manually\n",
        "    print(f\"\\n7. Manual Search Test:\")\n",
        "    test_image_id = filter_info['conflict_details'][0].get('image_id1')\n",
        "    test_ann_id = filter_info['conflict_details'][0].get('ann_id1')\n",
        "    \n",
        "    found = False\n",
        "    \n",
        "    # Test 1: Search by annotation_id\n",
        "    if test_ann_id and not found:\n",
        "        print(f\"   Searching for annotation_id={test_ann_id}...\")\n",
        "        try:\n",
        "            view = dataset.match(fo.ViewField(\"annotation_id\") == str(test_ann_id))\n",
        "            if len(view) > 0:\n",
        "                print(f\"   âœ“ FOUND using field 'annotation_id': {len(view)} sample(s)\")\n",
        "                sample = view.first()\n",
        "                print(f\"     Sample ID: {sample.id}\")\n",
        "                print(f\"     Annotation ID: {sample.annotation_id}\")\n",
        "                if hasattr(sample, 'image_id'):\n",
        "                    print(f\"     Image ID: {sample.image_id}\")\n",
        "                found = True\n",
        "            else:\n",
        "                print(f\"   âœ— Not found using field 'annotation_id'\")\n",
        "        except Exception as e:\n",
        "            print(f\"   âœ— Error with field 'annotation_id': {e}\")\n",
        "    \n",
        "    # Test 2: Search by image_id (as string)\n",
        "    if test_image_id and not found:\n",
        "        print(f\"   Searching for image_id={test_image_id} (as string)...\")\n",
        "        try:\n",
        "            view = dataset.match(fo.ViewField(\"image_id\") == str(test_image_id))\n",
        "            if len(view) > 0:\n",
        "                print(f\"   âœ“ FOUND using field 'image_id' (string): {len(view)} sample(s)\")\n",
        "                sample = view.first()\n",
        "                print(f\"     Sample ID: {sample.id}\")\n",
        "                if hasattr(sample, 'annotation_id'):\n",
        "                    print(f\"     Annotation ID: {sample.annotation_id}\")\n",
        "                if hasattr(sample, 'image_id'):\n",
        "                    print(f\"     Image ID: {sample.image_id}\")\n",
        "                found = True\n",
        "            else:\n",
        "                print(f\"   âœ— Not found using field 'image_id' (string)\")\n",
        "        except Exception as e:\n",
        "            print(f\"   âœ— Error with field 'image_id' (string): {e}\")\n",
        "    \n",
        "    # Test 3: Try other fields\n",
        "    if test_image_id and not found:\n",
        "        print(f\"   Searching for image_id={test_image_id} (trying other fields)...\")\n",
        "        \n",
        "        for field in ['coco_id', 'metadata.image_id', 'id']:\n",
        "            try:\n",
        "                view = dataset.match(fo.ViewField(field) == int(test_image_id))\n",
        "                if len(view) > 0:\n",
        "                    print(f\"   âœ“ FOUND using field '{field}': {len(view)} sample(s)\")\n",
        "                    sample = view.first()\n",
        "                    print(f\"     Sample ID: {sample.id}\")\n",
        "                    if hasattr(sample, 'detections'):\n",
        "                        print(f\"     Detections: {len(sample.detections.detections) if sample.detections else 0}\")\n",
        "                    found = True\n",
        "                    break\n",
        "                else:\n",
        "                    print(f\"   âœ— Not found using field '{field}'\")\n",
        "            except Exception as e:\n",
        "                print(f\"   âœ— Error with field '{field}': {e}\")\n",
        "    \n",
        "    if not found:\n",
        "        print(f\"\\n   âš ï¸ Could not find any samples. The function should now work correctly!\")\n",
        "                \n",
        "else:\n",
        "    print(\"No conflict info available. Run the conflict filtering first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ðŸŽ¨ NEW: Visualize Conflicts with Color-Coded Pairs\n",
        "\n",
        "Now with unique colored tags for each conflict pair!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # ðŸŽ¨ STEP 1: Create conflict view with unique colored tags\n",
        "if filter_info and len(filter_info['conflict_details']) > 0:\n",
        "    conflict_view = create_conflict_view_in_fiftyone(\n",
        "        filter_info['conflict_details'],\n",
        "        CONFIG['dataset_name'],\n",
        "        max_conflicts=50  # Show top 50 conflicts\n",
        "    )\n",
        "    \n",
        "    if len(conflict_view) > 0:\n",
        "        print(f\"\\nâœ… Successfully created view with {len(conflict_view)} samples\")\n",
        "        \n",
        "        # ðŸŽ¨ STEP 2: Create organized view sorted by pairs\n",
        "        sorted_view = create_conflict_pairs_visualization(conflict_view)\n",
        "        \n",
        "        # ðŸŽ¨ STEP 3: Launch FiftyOne App\n",
        "        print(\"\\nðŸš€ Launching FiftyOne App...\")\n",
        "        session = fo.launch_app(sorted_view)\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"FIFTYONE APP TIPS:\")\n",
        "        print(\"=\" * 80)\n",
        "        print(\"\\nðŸ” Filtering:\")\n",
        "        print(\"   â€¢ All conflicts: tag = 'conflict'\")\n",
        "        print(\"   â€¢ Specific pair: tag = 'conflict_pair_01' (or 02, 03, etc.)\")\n",
        "        print(\"\\nðŸ“Š Sorting:\")\n",
        "        print(\"   â€¢ Sort by 'conflict_pair_number' to see pairs together\")\n",
        "        print(\"   â€¢ Sort by 'conflict_similarity' to see most similar first\")\n",
        "        print(\"\\nðŸŽ¨ Color Coding:\")\n",
        "        print(\"   â€¢ Check 'conflict_color' field for each pair's unique color\")\n",
        "        print(\"   â€¢ Same color = same conflict pair\")\n",
        "        print(\"\\nðŸ“ Fields to Check:\")\n",
        "        print(\"   â€¢ conflict_own_label: This image's class\")\n",
        "        print(\"   â€¢ conflict_pair_label: What it conflicts with\")\n",
        "        print(\"   â€¢ conflict_similarity: How similar (higher = more similar)\")\n",
        "        print(\"=\" * 80)\n",
        "    else:\n",
        "        print(\"\\nâš ï¸ No conflicts found in the view.\")\n",
        "else:\n",
        "    print(\"No conflict info available. Run conflict filtering first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ðŸ”Ž View Specific Conflict Pairs\n",
        "\n",
        "Use these commands to focus on specific pairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 1: View only conflict pair #1\n",
        "# pair_1_view = view_specific_pair(conflict_view, 1)\n",
        "# session = fo.launch_app(pair_1_view)\n",
        "\n",
        "# Example 2: View conflict pair #2\n",
        "# pair_2_view = view_specific_pair(conflict_view, 2)\n",
        "# session = fo.launch_app(pair_2_view)\n",
        "\n",
        "# Example 3: Iterate through all pairs\n",
        "# for pair_num in range(1, 11):  # View first 10 pairs\n",
        "#     print(f\"\\n{'='*80}\")\n",
        "#     pair_view = view_specific_pair(conflict_view, pair_num)\n",
        "#     if len(pair_view) > 0:\n",
        "#         # Uncomment to launch app for each pair:\n",
        "#         # session = fo.launch_app(pair_view)\n",
        "#         # input(\"Press Enter to continue to next pair...\")\n",
        "#         pass\n",
        "\n",
        "# Example 4: Get all samples from pairs with high similarity (>0.95)\n",
        "# high_sim_view = conflict_view.match(fo.ViewField(\"conflict_similarity\") > 0.95)\n",
        "# print(f\"Found {len(high_sim_view)} samples with similarity > 0.95\")\n",
        "# session = fo.launch_app(high_sim_view)\n",
        "\n",
        "# Example 5: Group by conflict between specific classes\n",
        "# If you want to see all conflicts between two specific classes:\n",
        "# class1 = \"Thunnus albacares\"\n",
        "# class2 = \"Thunnus obesus\"\n",
        "# specific_conflict = conflict_view.match(\n",
        "#     (fo.ViewField(\"conflict_own_label\") == class1) & \n",
        "#     (fo.ViewField(\"conflict_pair_label\") == class2) |\n",
        "#     (fo.ViewField(\"conflict_own_label\") == class2) & \n",
        "#     (fo.ViewField(\"conflict_pair_label\") == class1)\n",
        "# )\n",
        "# print(f\"Conflicts between {class1} and {class2}: {len(specific_conflict)}\")\n",
        "# session = fo.launch_app(specific_conflict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸŽ¨ Summary: Color-Coded Conflict Visualization\n",
        "\n",
        "### What's New:\n",
        "\n",
        "1. **ðŸ·ï¸ Unique Tags per Pair**\n",
        "   - Each conflict pair gets a unique tag: `conflict_pair_01`, `conflict_pair_02`, etc.\n",
        "   - Easy filtering in FiftyOne App\n",
        "\n",
        "2. **ðŸŽ¨ Color Coding**\n",
        "   - Each pair has a unique color (stored in `conflict_color` field)\n",
        "   - Generated using HSV color space for maximum visual distinction\n",
        "   - 50 unique colors for 50 pairs\n",
        "\n",
        "3. **ðŸ“Š Rich Metadata**\n",
        "   - `conflict_pair_id`: Unique identifier (e.g., \"pair_01\")\n",
        "   - `conflict_pair_number`: Numeric ID (1, 2, 3, ...)\n",
        "   - `conflict_own_label`: This sample's class\n",
        "   - `conflict_pair_label`: Conflicting sample's class\n",
        "   - `conflict_similarity`: Similarity score (0-1)\n",
        "   - `conflict_role`: 'A' or 'B' (position in pair)\n",
        "   - `conflict_color`: Hex color code for visualization\n",
        "\n",
        "4. **ðŸ”§ Helper Functions**\n",
        "   - `create_conflict_pairs_visualization()`: Shows summary of all pairs\n",
        "   - `view_specific_pair()`: Focus on a single pair\n",
        "\n",
        "### Benefits:\n",
        "\n",
        "âœ… **Easy Identification**: Instantly see which images belong to the same conflict pair  \n",
        "âœ… **Organized View**: Sort by pair number to see pairs side-by-side  \n",
        "âœ… **Quick Filtering**: Use unique tags to focus on specific pairs  \n",
        "âœ… **Visual Distinction**: Each pair has its own color  \n",
        "âœ… **Detailed Analysis**: Rich metadata for each conflict\n",
        "\n",
        "### Workflow:\n",
        "\n",
        "```\n",
        "1. Run conflict filtering â†’ Identifies conflicts\n",
        "2. Create conflict view â†’ Tags with colors\n",
        "3. Launch FiftyOne App â†’ Visual inspection\n",
        "4. Filter by pair tag â†’ Focus on specific conflicts\n",
        "5. Analyze and fix â†’ Update dataset\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step-by-Step: Using FiftyOne Visualization\n",
        "\n",
        "**Recommended approach:**\n",
        "\n",
        "1. **First**, run the diagnostic cell above to understand your dataset structure\n",
        "2. **Then**, uncomment and run the visualization code below\n",
        "3. **If view is empty**, check the diagnostic output and modify the function accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if filter_info and len(filter_info['conflict_details']) > 0:\n",
        "    conflict_view = create_conflict_view_in_fiftyone(\n",
        "        filter_info['conflict_details'],\n",
        "        CONFIG['dataset_name'],\n",
        "        max_conflicts=50\n",
        "    )\n",
        "    \n",
        "    # Launch FiftyOne App\n",
        "    session = fo.launch_app(conflict_view)\n",
        "    print(\"\\nInspect conflicts in FiftyOne App!\")\n",
        "    print(\"Look at samples with tag:'conflict' and check 'conflict_pair_label' field\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8b. Weighted Fusion: Grid Search for Optimal Weights\n",
        "\n",
        "This section finds the optimal weights for combining ArcFace and kNN predictions:\n",
        "- **Weighted Fusion**: `final_score = Î± * arcface_score + Î² * knn_score`\n",
        "- Grid search over different Î± values (Î² = 1 - Î±)\n",
        "- Evaluate on validation set\n",
        "- Compare with other reranking methods (RRF, hybrid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load validation dataset\n",
        "print(\"Loading validation dataset...\")\n",
        "val_dataset = fo.load_dataset('classification_v0.10_val')\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "# Prepare data for grid search (sample subset for faster iteration)\n",
        "sample_size = min(1000, len(val_dataset))  # Use subset for faster tuning\n",
        "val_samples = val_dataset.take(sample_size)\n",
        "\n",
        "print(f\"\\nUsing {sample_size} validation samples for weight optimization\")\n",
        "print(\"This will take a few minutes...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_weighted_fusion(alpha, beta, val_samples, classifier):\n",
        "    \"\"\"\n",
        "    Evaluate accuracy with given weights for weighted fusion.\n",
        "    \n",
        "    Args:\n",
        "        alpha: Weight for ArcFace scores\n",
        "        beta: Weight for kNN scores\n",
        "        val_samples: Validation dataset samples\n",
        "        classifier: EmbeddingClassifier instance\n",
        "        \n",
        "    Returns:\n",
        "        accuracy: Top-1 accuracy\n",
        "        top5_accuracy: Top-5 accuracy\n",
        "    \"\"\"\n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "    \n",
        "    # Temporarily update classifier weights\n",
        "    original_alpha = classifier.arcface_weight\n",
        "    original_beta = classifier.knn_weight\n",
        "    original_mode = classifier.rerank_mode\n",
        "    \n",
        "    classifier.arcface_weight = alpha\n",
        "    classifier.knn_weight = beta\n",
        "    classifier.rerank_mode = 'weighted_fusion'\n",
        "    \n",
        "    correct_top1 = 0\n",
        "    correct_top5 = 0\n",
        "    total = 0\n",
        "    \n",
        "    for sample in val_samples:\n",
        "        try:\n",
        "            # Load image\n",
        "            image = Image.open(sample.filepath)\n",
        "            image_array = np.array(image)\n",
        "            \n",
        "            # Get predictions\n",
        "            results = classifier(image_array)\n",
        "            \n",
        "            # Ground truth\n",
        "            gt_label = sample.polyline.label\n",
        "            \n",
        "            # Top-1\n",
        "            if results[0].name == gt_label:\n",
        "                correct_top1 += 1\n",
        "            \n",
        "            # Top-5\n",
        "            top5_names = [r.name for r in results[:5]]\n",
        "            if gt_label in top5_names:\n",
        "                correct_top5 += 1\n",
        "            \n",
        "            total += 1\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing sample: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # Restore original weights\n",
        "    classifier.arcface_weight = original_alpha\n",
        "    classifier.knn_weight = original_beta\n",
        "    classifier.rerank_mode = original_mode\n",
        "    \n",
        "    top1_acc = correct_top1 / total if total > 0 else 0\n",
        "    top5_acc = correct_top5 / total if total > 0 else 0\n",
        "    \n",
        "    return top1_acc, top5_acc\n",
        "\n",
        "print(\"âœ… Evaluation function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize classifier (needed for grid search)\n",
        "from train_scripts.classification.interpreter_classifier_lightning import EmbeddingClassifier\n",
        "\n",
        "config = {\n",
        "    'log_level': 'CRITICAL',  # Reduce verbosity\n",
        "    'dataset': {'path': Path(CONFIG['output_dir']) / 'embedding_database_beitv2_top100.pt'},\n",
        "    'model': {\n",
        "        'checkpoint_path': CONFIG['checkpoint_path'],\n",
        "        'backbone_model_name': 'beitv2_base_patch16_224.in1k_ft_in22k_in1k',\n",
        "        'embedding_dim': 512,\n",
        "        'num_classes': len(train_class_to_id),\n",
        "        'arcface_s': 64.0,\n",
        "        'arcface_m': 0.2,\n",
        "        'pooling_type': 'attention',\n",
        "        'device': CONFIG['device']\n",
        "    },\n",
        "    'use_knn': True,\n",
        "    'rerank_mode': 'weighted_fusion',\n",
        "}\n",
        "\n",
        "print(\"Initializing classifier...\")\n",
        "classifier = EmbeddingClassifier(config)\n",
        "print(\"âœ… Classifier initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grid Search for optimal weights\n",
        "print(\"=\" * 60)\n",
        "print(\"GRID SEARCH: Finding Optimal Weights\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Define search space\n",
        "alphas = np.linspace(0, 1, 21)  # 0.0, 0.05, 0.1, ..., 1.0\n",
        "results_grid = []\n",
        "\n",
        "best_top1 = 0\n",
        "best_top5 = 0\n",
        "best_alpha = 0\n",
        "best_beta = 0\n",
        "\n",
        "print(f\"\\nTesting {len(alphas)} weight combinations...\")\n",
        "print(f\"{'Alpha':>6} {'Beta':>6} {'Top-1':>8} {'Top-5':>8}\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "for alpha in tqdm(alphas, desc=\"Grid Search\"):\n",
        "    beta = 1 - alpha\n",
        "    \n",
        "    # Evaluate\n",
        "    top1_acc, top5_acc = evaluate_weighted_fusion(alpha, beta, val_samples, classifier)\n",
        "    \n",
        "    results_grid.append({\n",
        "        'alpha': alpha,\n",
        "        'beta': beta,\n",
        "        'top1_accuracy': top1_acc,\n",
        "        'top5_accuracy': top5_acc\n",
        "    })\n",
        "    \n",
        "    # Track best\n",
        "    if top1_acc > best_top1:\n",
        "        best_top1 = top1_acc\n",
        "        best_top5 = top5_acc\n",
        "        best_alpha = alpha\n",
        "        best_beta = beta\n",
        "    \n",
        "    # Print progress\n",
        "    print(f\"{alpha:6.2f} {beta:6.2f} {top1_acc:8.4f} {top5_acc:8.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"BEST WEIGHTS FOUND:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  Î± (ArcFace weight): {best_alpha:.4f}\")\n",
        "print(f\"  Î² (kNN weight):     {best_beta:.4f}\")\n",
        "print(f\"  Top-1 Accuracy:     {best_top1:.4f} ({best_top1*100:.2f}%)\")\n",
        "print(f\"  Top-5 Accuracy:     {best_top5:.4f} ({best_top5*100:.2f}%)\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Grid Search Results\n",
        "df_grid = pd.DataFrame(results_grid)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Top-1 Accuracy vs Alpha\n",
        "axes[0].plot(df_grid['alpha'], df_grid['top1_accuracy'], 'b-o', linewidth=2, markersize=6)\n",
        "axes[0].axvline(best_alpha, color='r', linestyle='--', label=f'Best Î±={best_alpha:.2f}')\n",
        "axes[0].axhline(best_top1, color='r', linestyle=':', alpha=0.3)\n",
        "axes[0].set_xlabel('Î± (ArcFace Weight)', fontsize=12)\n",
        "axes[0].set_ylabel('Top-1 Accuracy', fontsize=12)\n",
        "axes[0].set_title('Weighted Fusion: Top-1 Accuracy vs Weight', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].legend()\n",
        "\n",
        "# Plot 2: Top-5 Accuracy vs Alpha\n",
        "axes[1].plot(df_grid['alpha'], df_grid['top5_accuracy'], 'g-s', linewidth=2, markersize=6)\n",
        "axes[1].axvline(best_alpha, color='r', linestyle='--', label=f'Best Î±={best_alpha:.2f}')\n",
        "axes[1].axhline(best_top5, color='r', linestyle=':', alpha=0.3)\n",
        "axes[1].set_xlabel('Î± (ArcFace Weight)', fontsize=12)\n",
        "axes[1].set_ylabel('Top-5 Accuracy', fontsize=12)\n",
        "axes[1].set_title('Weighted Fusion: Top-5 Accuracy vs Weight', fontsize=14, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(Path(CONFIG['output_dir']) / 'weighted_fusion_grid_search.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ… Visualization saved to:\", Path(CONFIG['output_dir']) / 'weighted_fusion_grid_search.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare Reranking Methods\n",
        "\n",
        "Now compare weighted fusion with optimal weights vs other methods (RRF, Hybrid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare different reranking methods\n",
        "print(\"=\" * 60)\n",
        "print(\"COMPARISON: Different Reranking Methods\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "methods_to_test = [\n",
        "    ('Weighted Fusion (Optimal)', 'weighted_fusion', {'alpha': best_alpha, 'beta': best_beta}),\n",
        "    ('Weighted Fusion (Equal)', 'weighted_fusion', {'alpha': 0.5, 'beta': 0.5}),\n",
        "    ('Weighted Fusion (ArcFace Priority)', 'weighted_fusion', {'alpha': 0.7, 'beta': 0.3}),\n",
        "    ('Weighted Fusion (kNN Priority)', 'weighted_fusion', {'alpha': 0.3, 'beta': 0.7}),\n",
        "    ('Reciprocal Rank Fusion', 'rrf', {}),\n",
        "    ('Hybrid (Original)', 'hybrid', {}),\n",
        "]\n",
        "\n",
        "comparison_results = []\n",
        "\n",
        "for method_name, mode, params in methods_to_test:\n",
        "    print(f\"\\nTesting: {method_name}\")\n",
        "    \n",
        "    # Configure classifier\n",
        "    classifier.rerank_mode = mode\n",
        "    if 'alpha' in params:\n",
        "        classifier.arcface_weight = params['alpha']\n",
        "        classifier.knn_weight = params['beta']\n",
        "    \n",
        "    # Evaluate\n",
        "    correct_top1 = 0\n",
        "    correct_top5 = 0\n",
        "    total = 0\n",
        "    \n",
        "    for sample in tqdm(val_samples, desc=method_name, leave=False):\n",
        "        try:\n",
        "            from PIL import Image\n",
        "            import numpy as np\n",
        "            \n",
        "            image = Image.open(sample.filepath)\n",
        "            image_array = np.array(image)\n",
        "            results = classifier(image_array)\n",
        "            \n",
        "            gt_label = sample.polyline.label\n",
        "            \n",
        "            # Top-1\n",
        "            if results[0].name == gt_label:\n",
        "                correct_top1 += 1\n",
        "            \n",
        "            # Top-5\n",
        "            top5_names = [r.name for r in results[:5]]\n",
        "            if gt_label in top5_names:\n",
        "                correct_top5 += 1\n",
        "            \n",
        "            total += 1\n",
        "        except Exception as e:\n",
        "            continue\n",
        "    \n",
        "    top1_acc = correct_top1 / total if total > 0 else 0\n",
        "    top5_acc = correct_top5 / total if total > 0 else 0\n",
        "    \n",
        "    comparison_results.append({\n",
        "        'method': method_name,\n",
        "        'mode': mode,\n",
        "        'top1_accuracy': top1_acc,\n",
        "        'top5_accuracy': top5_acc,\n",
        "        'params': str(params)\n",
        "    })\n",
        "    \n",
        "    print(f\"  Top-1: {top1_acc:.4f} ({top1_acc*100:.2f}%)\")\n",
        "    print(f\"  Top-5: {top5_acc:.4f} ({top5_acc*100:.2f}%)\")\n",
        "\n",
        "# Create comparison DataFrame\n",
        "df_comparison = pd.DataFrame(comparison_results)\n",
        "df_comparison = df_comparison.sort_values('top1_accuracy', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"RANKING BY TOP-1 ACCURACY:\")\n",
        "print(\"=\" * 60)\n",
        "print(df_comparison[['method', 'top1_accuracy', 'top5_accuracy']].to_string(index=False))\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize comparison\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "x = np.arange(len(df_comparison))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax.bar(x - width/2, df_comparison['top1_accuracy'], width, \n",
        "               label='Top-1 Accuracy', alpha=0.8, color='steelblue')\n",
        "bars2 = ax.bar(x + width/2, df_comparison['top5_accuracy'], width, \n",
        "               label='Top-5 Accuracy', alpha=0.8, color='lightcoral')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.3f}',\n",
        "                ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "ax.set_xlabel('Reranking Method', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Comparison of Reranking Methods', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(df_comparison['method'], rotation=45, ha='right')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(Path(CONFIG['output_dir']) / 'reranking_methods_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ… Comparison visualization saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ“Š Summary & Recommendations\n",
        "\n",
        "Based on the grid search results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save optimal configuration\n",
        "optimal_config = {\n",
        "    'reranking': {\n",
        "        'method': 'weighted_fusion',\n",
        "        'optimal_weights': {\n",
        "            'arcface_weight': float(best_alpha),\n",
        "            'knn_weight': float(best_beta)\n",
        "        },\n",
        "        'performance': {\n",
        "            'top1_accuracy': float(best_top1),\n",
        "            'top5_accuracy': float(best_top5),\n",
        "            'validation_samples': len(val_samples)\n",
        "        }\n",
        "    },\n",
        "    'comparison': comparison_results\n",
        "}\n",
        "\n",
        "# Save to JSON\n",
        "config_path = Path(CONFIG['output_dir']) / 'optimal_reranking_config.json'\n",
        "with open(config_path, 'w') as f:\n",
        "    json.dump(optimal_config, f, indent=2)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"âœ… OPTIMAL CONFIGURATION SAVED\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Location: {config_path}\")\n",
        "print(f\"\\nRecommended config for production:\")\n",
        "print(f\"```python\")\n",
        "print(f\"config = {{\")\n",
        "print(f\"    'rerank_mode': 'weighted_fusion',\")\n",
        "print(f\"    'arcface_weight': {best_alpha:.4f},\")\n",
        "print(f\"    'knn_weight': {best_beta:.4f},\")\n",
        "print(f\"}}\")\n",
        "print(f\"```\")\n",
        "print(f\"\\nExpected performance:\")\n",
        "print(f\"  Top-1 Accuracy: {best_top1*100:.2f}%\")\n",
        "print(f\"  Top-5 Accuracy: {best_top5*100:.2f}%\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ”¬ Advanced Tips & Best Practices\n",
        "\n",
        "### Interpreting Conflict Results\n",
        "\n",
        "**High conflict rate (>5%):**\n",
        "- May indicate systematic labeling issues\n",
        "- Consider increasing similarity threshold (0.97-0.99)\n",
        "- Manually review top conflicts in FiftyOne\n",
        "- Check if certain annotators/sources have more errors\n",
        "\n",
        "**Low conflict rate (<0.5%):**\n",
        "- Good data quality! \n",
        "- Can try lower threshold (0.90-0.93) for more aggressive cleaning\n",
        "- May still have subtle labeling errors\n",
        "\n",
        "### Common Conflict Patterns\n",
        "\n",
        "1. **Near-Identical Species**\n",
        "   - Example: Juvenile vs Adult of same species\n",
        "   - Solution: May be valid, check biological accuracy\n",
        "   \n",
        "2. **Similar Looking Species**\n",
        "   - Example: Two tuna species with similar appearance\n",
        "   - Solution: True labeling challenge, may need expert review\n",
        "   \n",
        "3. **Data Collection Errors**\n",
        "   - Example: Same image uploaded multiple times with different labels\n",
        "   - Solution: These should be removed (true duplicates)\n",
        "   \n",
        "4. **Annotation Mistakes**\n",
        "   - Example: Misclicked label during annotation\n",
        "   - Solution: Fix in dataset, re-export\n",
        "\n",
        "### Next Steps After Conflict Filtering\n",
        "\n",
        "1. **Review conflict_report.csv**\n",
        "   - Sort by similarity (highest first)\n",
        "   - Manually check top 20-50 conflicts\n",
        "   - Identify patterns\n",
        "\n",
        "2. **Fix Dataset Issues**\n",
        "   - For confirmed errors, correct labels in your dataset\n",
        "   - Remove true duplicates\n",
        "   - Re-export and re-run training\n",
        "\n",
        "3. **Monitor Impact**\n",
        "   - Compare model performance before/after filtering\n",
        "   - Check if problematic class pairs improve\n",
        "   - Validate on held-out test set\n",
        "\n",
        "4. **Iterate**\n",
        "   - Adjust threshold based on results\n",
        "   - Re-run conflict filtering after dataset fixes\n",
        "   - Track improvement over time"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
